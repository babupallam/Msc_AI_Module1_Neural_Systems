{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNEZ2eoGdvUKtLfY9XB4TZO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babupallam/Msc_AI_Module1_Neural_Systems/blob/main/Supporting%20Research/Partial%20Connectivity/02_Paper_Review_Non_Ontogenic_Sparse_Neural_Networks_David_A_Elizondo_and_Others.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Title:** Non-Ontogenic Sparse Neural Networks\n",
        "\n",
        "**Authors:** D. Elizondo, E. Fiesler, and J. Korczak\n",
        "\n",
        "**Aim of the Paper:**\n",
        "The paper presents an extensive study on non-ontogenic sparse neural networks (SCNNs), which are defined by their static topology that remains unchanged during the learning process. It aims to explore various methods for achieving sparse connectivity in neural networks and discuss the advantages over fully connected neural networks (FCNNs).\n",
        "\n",
        "**What It Proposes:**\n",
        "- The paper proposes an in-depth classification of non-ontogenic SCNNs based on different methodologies:\n",
        "  - **Theoretical and Experimental Studies:** Focuses on training dynamics and performance in sparse networks.\n",
        "  - **Biological Neural Networks:** Connectivity strategies inspired by biological systems.\n",
        "  - **Application Dependent Methods:** Techniques tailored to specific tasks, such as image processing and natural language processing.\n",
        "  - **Modular Networks:** Divide complex tasks into smaller sub-networks.\n",
        "  - **Hardware Implementation:** Explores partial connectivity strategies for analog and digital implementations of neural networks.\n",
        "  - **Hybrid Methods:** Combines symbolic knowledge and neural networks, as well as genetic programming to optimize network topology.\n",
        "\n",
        "**Key Points:**\n",
        "- **Advantages of SCNNs:**\n",
        "  - Reduced training and recall time.\n",
        "  - Improved generalization capabilities.\n",
        "  - Reduced hardware requirements.\n",
        "  - A closer resemblance to biological neural systems.\n",
        "  \n",
        "- **Approaches:** Various methods are discussed, including pruning connections, modularity, random selection of connections, and biological connectivity patterns.\n",
        "  \n",
        "**Conclusion:**\n",
        "Non-ontogenic sparse neural networks offer several benefits compared to fully connected ones, particularly in terms of efficiency and generalization. The paper categorizes different approaches to achieving sparse connectivity, providing a comprehensive view of the field.\n"
      ],
      "metadata": {
        "id": "vTeUzjVq78QA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detailed Review"
      ],
      "metadata": {
        "id": "TfxOjvfy9YxK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Introduction**\n",
        "\n",
        "#### **1.1 Overview of Fully Connected Neural Networks (FCNNs)**\n",
        "- **Definition:** Fully connected neural networks (FCNNs) are the default configuration for most artificial neural networks, meaning every neuron is connected to every other neuron in subsequent layers.\n",
        "- **Drawbacks:**\n",
        "  - **High Complexity:** FCNNs often result in high complexity and redundancy in connections.\n",
        "  - **Training Overhead:** Leads to longer training and recall times.\n",
        "  - **Decreased Generalization:** The redundancy can negatively impact the model’s ability to generalize to new data.\n",
        "\n",
        "#### **1.2 The Concept of Sparse Neural Networks (SCNNs)**\n",
        "- **Definition:** SCNNs are networks with fewer connections, either designed from the start or modified during training to remove unnecessary connections.\n",
        "- **Advantages of SCNNs:**\n",
        "  - **Reduced Training and Recall Time:** Less complexity results in faster computations.\n",
        "  - **Improved Generalization:** Sparse connections reduce the risk of overfitting.\n",
        "  - **Lower Hardware Requirements:** Fewer connections mean less memory and hardware resources are required.\n",
        "  - **Closer to Biological Networks:** SCNNs more closely resemble biological neural networks, which are sparsely connected.\n",
        "\n",
        "#### **1.3 Ontogenic vs. Non-Ontogenic Methods**\n",
        "- **Ontogenic Methods:** These modify the network topology during the learning phase, typically by pruning or adding connections as part of the learning process.\n",
        "- **Non-Ontogenic Methods:** The focus of this paper. These methods define the network’s topology before training begins, and it remains static throughout the learning process.\n",
        "  - **Emphasis on Non-Ontogenic SCNNs:** The paper will not cover ontogenic methods extensively and instead focuses on static, pre-defined sparse topologies.\n",
        "\n",
        "#### **1.4 Classification of Non-Ontogenic SCNN Methods**\n",
        "- **Theoretical and Experimental Studies:**\n",
        "  - Focus on demonstrating the effects of sparse connectivity on training dynamics.\n",
        "  - Typically theoretical, with limited application testing.\n",
        "- **Methods Derived from Biological Neural Networks:**\n",
        "  - Inspired by how biological neurons connect sparsely in real-life systems.\n",
        "  - Based on biological principles like synaptic growth and random connectivity patterns.\n",
        "- **Application-Specific Methods:**\n",
        "  - Designed for specific tasks, such as speech recognition or image processing.\n",
        "  - These methods are tailored to specific problem domains, reducing the network’s complexity by focusing on problem-specific characteristics.\n",
        "- **Modular Networks:**\n",
        "  - Involves dividing the problem into smaller sub-tasks, with each sub-task being handled by a specialized sub-network.\n",
        "  - The sub-networks are combined to solve the overall problem.\n",
        "- **Hardware Implementation Methods:**\n",
        "  - Focus on how to implement SCNNs efficiently using analog or digital hardware, overcoming the limitations of full connectivity in large-scale hardware systems.\n",
        "- **Hybrid Methods Combining Neural Networks and Inductive Knowledge:**\n",
        "  - Use domain-specific symbolic knowledge to define initial network topology.\n",
        "  - The network is then refined through learning algorithms to improve performance.\n"
      ],
      "metadata": {
        "id": "I3s-VJfI78UG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Non-Ontogenic Methods**\n",
        "\n",
        "#### **2.1 Methods Based on Theoretical and Experimental Studies**\n",
        "\n",
        "- **Objective:** To explore and demonstrate the consequences of partially connected neural networks (PCNNs) in terms of their training dynamics. These studies focus on the effects of sparse connectivity in neural networks rather than applying them to real-world problems.\n",
        "  \n",
        "- **Approaches:**\n",
        "  - **Random Selection:** Arbitrarily selecting connections between neurons, often within a specific neighborhood.\n",
        "  - **Weight Dilution:** Reducing connections by randomly cutting them within a predefined probability.\n",
        "  - **Varying Connectivity Levels:** Experimenting with minimal to full connectivity to find the optimal level that yields the best performance.\n",
        "  - **Exhaustive Methods:** Testing all possible network topologies.\n",
        "  - **High Order Connections:** Introducing high-order connections for better model performance.\n",
        "\n",
        "- **Findings:**\n",
        "  - There is an optimal connectivity level that depends on the problem being solved. Reduced connectivity can lead to better performance in specific scenarios.\n",
        "  \n",
        "#### **2.2 Methods Derived from Biological Neural Networks**\n",
        "\n",
        "- **Focus:** These methods mimic the sparse connectivity observed in biological neural systems. The goal is to study the storage capacity and dynamics of biologically inspired PCNNs.\n",
        "  \n",
        "- **Connectivity Strategies:**\n",
        "  - **Neural Spike Simulations:** Models based on neural spike activity.\n",
        "  - **Statistical Measures:** Using probabilistic models and distributions.\n",
        "  - **Synaptic Growth Models:** Non-Hebbian processes that replicate synaptic growth in biological systems.\n",
        "  - **Random Connection Selection:** Selecting connections randomly to imitate biological systems.\n",
        "\n",
        "#### **2.3 Application-Dependent Methods**\n",
        "\n",
        "- **Purpose:** Tailored to specific applications such as speech recognition and image processing.\n",
        "  \n",
        "- **Connectivity Approaches:**\n",
        "  - **Local Connectivity:** Only connecting nearby neurons.\n",
        "  - **Geometrical Relationships:** Defining connections based on the geometric patterns in input data.\n",
        "  - **Shared Weights:** Using shared weights across neurons to reduce redundancy.\n",
        "  \n",
        "- **Limitations:** These methods are typically only applicable within their respective domains and are not generalizable to other problem areas.\n",
        "\n",
        "#### **2.4 Modular-Based PCNNs**\n",
        "\n",
        "- **Concept:** Task division into smaller, more manageable sub-tasks. Each sub-task is handled by a sub-network, which is later combined to solve the overall problem.\n",
        "  \n",
        "- **Challenges:**\n",
        "  - **Task Decomposition:** Critical for performance. Often, ad-hoc techniques are required for decomposing tasks in function approximation problems.\n",
        "\n",
        "#### **2.5 PCNNs for Hardware Implementation**\n",
        "\n",
        "- **Focus:** Aimed at overcoming physical limitations in hardware, such as VLSI technology.\n",
        "  \n",
        "- **Advantages:** Partially connected networks help to reduce hardware complexity, improving both training and recall times.\n",
        "  \n",
        "- **Methods:**\n",
        "  - Restructuring fully connected networks into partially connected ones to optimize hardware usage.\n",
        "  - Cellular neural networks (CNNs) with local connectivity to nearby neurons.\n"
      ],
      "metadata": {
        "id": "KWQ7qDXn78Xy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Ontogenic Methods**\n",
        "\n",
        "#### **3.1 Multilayer Perceptron-Based Methods**\n",
        "\n",
        "- **Multilayer Perceptron (MLP):** One of the most popular neural networks used in supervised learning. It consists of an input layer, output layer, and one or more hidden layers. Information flows in a feedforward manner from input to output.\n",
        "- **Error Backpropagation:** The most widely used training algorithm for MLPs, which adjusts the weights of the connections by propagating the error backward from the output layer. The learning process is based on gradient descent.\n",
        "  \n",
        "- **Types of Ontogenic Methods:**\n",
        "  - **Pruning Methods:**\n",
        "    - Initially, an oversized network is trained, and redundant or unnecessary connections are gradually removed.\n",
        "    - Some of the methods include:\n",
        "      - **Energy Functions**\n",
        "      - **Penalty Functions**\n",
        "      - **Optimal Brain Damage**\n",
        "      - **Weight Elimination**\n",
        "  - **Growing and Pruning Methods:**\n",
        "    - These methods combine adding new connections during training with pruning techniques.\n",
        "    - Examples include:\n",
        "      - **Units Recruitment**\n",
        "      - **Cutting and Creating Connections**\n",
        "\n",
        "#### **3.2 Other Ontogenic Methods**\n",
        "\n",
        "- **Non-MLP-Based Methods:**\n",
        "  - There are other ontogenic methods that apply different neural network learning rules.\n",
        "  - Methods include:\n",
        "    - **Monte Carlo procedures** for architecture optimization.\n",
        "    - **Limited fan-in connections** or **cascade-correlation methods**.\n"
      ],
      "metadata": {
        "id": "Jauvh-qi78a1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Hybrid Methods**\n",
        "\n",
        "#### **4.1 Knowledge-Based PCNNs**\n",
        "- **Objective:** Combine symbolic domain knowledge (e.g., rules) with neural networks to define the network's topology. The learning algorithm further refines the network based on empirical learning.\n",
        "- **Methodology:**\n",
        "  - The initial structure and weights of the neural network are defined by symbolic rules expressed as Horn clauses.\n",
        "  - After inserting the symbolic rules, the neural network is refined through standard learning algorithms such as backpropagation.\n",
        "- **Example:**\n",
        "  - **KBANN System:** A system that uses a set of approximate rules to determine the structure and initial weights of the neural network. These rules are refined post-training, and the knowledge is extracted from the network.\n",
        "  - **Drawbacks:**\n",
        "    - Additional hidden units are often needed to improve accuracy.\n",
        "    - New links might need to be added to discover previously unidentified dependencies.\n",
        "\n",
        "#### **4.2 Genetic Programming-Based Methods**\n",
        "- **Objective:** Utilize genetic algorithms (GA) to optimize both the topology of the neural network and the initial weights.\n",
        "- **Methodology:**\n",
        "  - **Genetic Algorithms (GAs):** A guided random search based on principles of natural selection and genetics, using operators like selection, crossover, mutation, and inversion.\n",
        "  - The algorithms begin with a random population and evolve toward better solutions over time.\n",
        "- **Key Focus:**\n",
        "  - The encoding of neural networks into chromosomes for GA optimization.\n",
        "  - Evaluating the fitness of individual solutions.\n",
        "- **Challenges:**\n",
        "  - **Excessive Computational Time:** Finding the optimal topology can be computationally expensive.\n",
        "  - **Solutions:** Suggestions to mitigate this issue include using specialized neural hardware and parallel computing techniques.\n"
      ],
      "metadata": {
        "id": "0SvUwhOn78cs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Summary and Conclusions**\n",
        "\n",
        "#### **5.1 Overview of Non-Ontogenic Methods**\n",
        "- **Theoretical and Experimental Studies:** These methods explore the effects of sparse connectivity in neural networks, typically in terms of training dynamics. They are often theoretical, with limited practical applications.\n",
        "  - **Methods Include:**\n",
        "    - Random connection selection within a local neighborhood.\n",
        "    - Connectivity levels varied from minimal to plenary.\n",
        "    - Use of random unit selection and weight dilution to reduce connectivity.\n",
        "- **Findings:** There is typically an optimal level of connectivity specific to the problem being addressed, which provides better performance with reduced complexity.\n",
        "\n",
        "#### **5.2 Ontogenic Methods**\n",
        "- **General Aim:** Ontogenic methods focus on dynamically adjusting the network topology during training to improve generalization and performance. These methods involve pruning or growing the network by modifying connections based on performance.\n",
        "  - **Key Approaches:**\n",
        "    - **Pruning connections:** Start with an oversized network and reduce connections to improve efficiency.\n",
        "    - **Growing methods:** Add new connections during training, combined with pruning methods to create an efficient structure.\n",
        "  \n",
        "#### **5.3 Hybrid Methods**\n",
        "- **Knowledge-Based PCNNs:**\n",
        "  - These methods are in the early stages but provide a foundation for defining a network’s topology based on symbolic knowledge (rules). The topology is refined through empirical learning.\n",
        "  - **Future Possibilities:** Integrating automatic rule extraction could enhance these methods by allowing networks to automatically learn rules from training data.\n",
        "- **Genetic Programming-Based Methods:**\n",
        "  - Genetic algorithms (GA) are used to optimize the topology and weights of neural networks.\n",
        "  - **Challenges:** Computational complexity remains a significant drawback. Solutions such as parallel computing and specialized hardware are proposed to mitigate this issue.\n",
        "\n",
        "#### **Conclusions:**\n",
        "- **Advantages of SCNNs:**\n",
        "  - Improved generalization capabilities.\n",
        "  - Reduced hardware requirements.\n",
        "  - Faster training and recall times.\n",
        "- **Challenges:**\n",
        "  - **Computational complexity** in hybrid and genetic programming-based methods.\n",
        "  - **Task-specific adaptation** in application-dependent methods.\n",
        "- **Future Directions:**\n",
        "  - Improving hybrid methods by integrating automatic rule extraction.\n",
        "  - Expanding the scope of genetic programming to reduce computational overhead.\n"
      ],
      "metadata": {
        "id": "zJImEjA578eA"
      }
    }
  ]
}