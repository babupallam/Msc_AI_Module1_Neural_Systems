{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMncs4LqT9hfhyynQDiLalu",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babupallam/Msc_AI_Module1_Neural_Systems/blob/main/Supporting%20Research/Partial%20Connectivity/01_Paper_Review_A_Survey_of_Partially_Connected_Neural_Networks_David_A_Elizondo_and_Others.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Title:** A Survey of Partially Connected Neural Networks\n",
        "\n",
        "**Authors:** D. Elizondo, E. Fiesler, J. Korczak\n",
        "\n",
        "**Aim of the Paper:** The paper aims to provide a comprehensive survey of partially connected neural networks (PCNNs), contrasting them with fully connected neural networks (FCNNs) and highlighting the advantages and research on PCNNs. The goal is to explore how reduced connectivity in neural networks can lead to benefits such as reduced training and recall time, improved generalization capabilities, reduced hardware requirements, and a closer resemblance to biological neural networks.\n",
        "\n",
        "**Proposals/Findings:**\n",
        "- **Partially Connected Neural Networks (PCNNs):** Defined as networks that contain only a subset of possible connections, reducing redundancy.\n",
        "- **Advantages:** PCNNs may offer reduced complexity, hardware needs, and training times while improving generalization.\n",
        "- **Methods Discussed:**\n",
        "  - **Ontogenic Methods:** These modify the topology of the network during learning.\n",
        "  - **Non-Ontogenic Methods:** These use a static topology defined before the learning phase, which does not change during learning.\n",
        "  - **Hybrid Methods:** These combine neural networks with artificial intelligence techniques like genetic programming.\n",
        "\n",
        "The paper organizes the research on PCNNs into different categories based on how connections are added or removed, providing a detailed discussion on their theoretical and practical implications.\n"
      ],
      "metadata": {
        "id": "vpG-zv3F76vY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detailed Review"
      ],
      "metadata": {
        "id": "NwYQepMq76zZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Here’s a comprehensive review of the first section of the paper:\n",
        "\n",
        "### **1. Introduction**\n",
        "\n",
        "#### 1.1 Overview of Fully Connected Neural Networks (FCNNs)\n",
        "- **Definition:** FCNNs are the most commonly used neural networks because they simplify neural network design.\n",
        "- **Redundancy:** FCNNs often include a large amount of redundant connections.\n",
        "- **Different Interpretations of FCNNs:**\n",
        "  - **Plenary neural networks:** Includes all possible interlayer, intralayer, supralayer, and self-connections.\n",
        "  - **Plenary networks without self-connections:** Typically used in associative memories.\n",
        "  - **Fully interlayer connected networks (FICNNs):** Have all interlayer connections, but no intralayer or supralayer connections.\n",
        "\n",
        "#### 1.2 Partially Connected Neural Networks (PCNNs)\n",
        "- **Definition:** PCNNs are neural networks with a subset of all possible connections in the network.\n",
        "- **Goal of PCNNs:** Achieve equivalent or better performance than FCNNs but with fewer connections, reducing:\n",
        "  - Network complexity\n",
        "  - Hardware and storage needs\n",
        "  - Training and recall time\n",
        "\n",
        "#### 1.3 Importance of PCNNs\n",
        "- **Generalization Improvement:** PCNNs can help improve the generalization ability of neural networks due to the reduction in complexity.\n",
        "- **Closer to Biological Reality:** The reduced connectivity in PCNNs more closely resembles the structure of biological neural networks.\n",
        "\n",
        "#### 1.4 Methods for Adding/Deleting Connections in PCNNs\n",
        "- **Three Main Classes of Methods:**\n",
        "  - **Ontogenic methods:** Modify network topology during the learning phase.\n",
        "  - **Non-ontogenic methods:** The emphasis of the paper; these methods do not change the topology during learning.\n",
        "  - **Hybrid methods:** Combine neural networks with other AI techniques like genetic programming.\n"
      ],
      "metadata": {
        "id": "qpahbpz67604"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **2. Non-Ontogenic Methods**\n",
        "\n",
        "#### 2.1 Theoretical and Experimental Studies on PCNNs\n",
        "\n",
        "- **Focus:** These methods aim to understand the consequences of sparse connectivity by examining training dynamics.\n",
        "- **Subsections:**\n",
        "  - **Gradient Descend-based Studies:**\n",
        "    - Hansen et al. investigate reducing the number of weights in networks dealing with ill-posed problems, where the number of inputs is high but training samples are few.\n",
        "    - Approach reduces the dimensionality of the problem, focusing on both supervised and unsupervised neural networks.\n",
        "  - **Optimal Number of Connections:**\n",
        "    - A study by Kung et al. explores the idea that synaptic weights are strongest among neighboring neurons and degrade with distance, using local connectivity with some global connections.\n",
        "\n",
        "#### 2.2 PCNNs Derived from Biological Neural Networks\n",
        "\n",
        "- **Focus:** These methods are inspired by biological neural networks, particularly in their sparse connectivity.\n",
        "- **Subsections:**\n",
        "  - **Connection Strategies:** Derived from neural spike activity, statistical probability, synaptic growth, and random selection.\n",
        "\n",
        "#### 2.3 Application Dependent PCNNs\n",
        "\n",
        "- **Focus:** PCNN models are tailored to specific tasks like speech recognition and image processing.\n",
        "- **Connectivity Methods:**\n",
        "  - A subset of consecutive units or nearest neighbors.\n",
        "  - Use of local or shared weights.\n",
        "  - Incorporation of high-order connections based on geometric input patterns.\n",
        "\n",
        "#### 2.4 Modular-based PCNNs\n",
        "\n",
        "- **Design:** Divides a task into sub-tasks, assigning each sub-task to a specialized sub-network with potentially different topologies.\n",
        "- **Performance Issues:** Task decomposition is critical for performance, and current approaches rely heavily on spatial clustering.\n",
        "\n",
        "#### 2.5 PCNNs for Hardware Implementation\n",
        "\n",
        "- **Motivation:** Focuses on physical limitations in hardware, like VLSI technology.\n",
        "- **Advantages:** Implementing PCNNs in hardware can drastically improve training and recall speeds.\n",
        "  "
      ],
      "metadata": {
        "id": "N7nbaT_7763n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **3. Ontogenic Methods**\n",
        "\n",
        "#### **3.1 Multilayer Perceptron Based Methods**\n",
        "- **Multilayer Perceptron (MLP):**\n",
        "  - The most common neural network used for supervised learning.\n",
        "  - Organized in layers with interlayer connections; information flows in a feedforward direction.\n",
        "  - Training typically uses the error backpropagation algorithm.\n",
        "- **Function of Backpropagation:**\n",
        "  - Adjusts connection weights by comparing output patterns to target patterns and propagating the error backward.\n",
        "  - The learning algorithm follows the gradient descent principle to adjust connection weights.\n",
        "  \n",
        "- **Categories of Ontogenic Methods Based on Backpropagation:**\n",
        "  - **Pruning Methods:**\n",
        "    - Initially train an oversized network and then remove unnecessary connections to achieve the smallest effective topology.\n",
        "    - Some pruning methods include:\n",
        "      - Energy functions\n",
        "      - Penalty function\n",
        "      - Optimal Brain Damage\n",
        "      - Ockham’s Razor\n",
        "      - Weight Elimination\n",
        "  - **Growing and Pruning Methods:**\n",
        "    - Combine the addition of connections during learning with pruning to optimize the network size.\n",
        "\n",
        "#### **3.2 Other Ontogenic Methods**\n",
        "- **Besides MLP-based methods:**\n",
        "  - There are other ontogenic methods that modify network topologies using non-backpropagation learning rules.\n",
        "  - Examples include methods using Monte Carlo procedures or other complex optimization strategies.\n"
      ],
      "metadata": {
        "id": "EmzNd7pZ767u"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **4. Hybrid Methods**\n",
        "\n",
        "#### **4.1 Knowledge-Based PCNNs**\n",
        "- **Combination of Symbolic Knowledge and Neural Networks:**\n",
        "  - Uses symbolic domain knowledge in the form of rules to create the network topology.\n",
        "  - The neural network is refined through a training algorithm to improve performance.\n",
        "- **KBANN System:**\n",
        "  - Combines explanation-based learning (rules) with empirical learning.\n",
        "  - Uses Horn clauses to define rules that determine both the structure and initial weights of the network.\n",
        "  - After training, the neural network refines the symbolic rules.\n",
        "  - Key Correspondences:\n",
        "    - Input units = Supporting facts.\n",
        "    - Hidden units = Intermediate conclusions.\n",
        "    - Output units = Final conclusions.\n",
        "  - **Drawbacks:**\n",
        "    - The user needs to define additional hidden units to improve network accuracy.\n",
        "    - New connections may be required between units at contiguous levels to discover dependencies not previously specified.\n",
        "\n",
        "#### **4.2 Genetic Programming-Based Methods**\n",
        "- **Genetic Algorithms (GA):**\n",
        "  - A type of guided random search algorithm based on natural selection and genetics.\n",
        "  - Operates using four primary operators: selection, crossover, inversion, and mutation.\n",
        "  - GA starts from a random population and evolves towards the best solution.\n",
        "- **Applications of GA in PCNNs:**\n",
        "  - Used to optimize both the topology of the neural network and the initial set of weights.\n",
        "  - The methods differ in how they encode the network into GA chromosomes and evaluate the fitness of individuals.\n",
        "  - **Challenges:**\n",
        "    - Computational time is a significant challenge in GA-based methods, especially in optimizing network topology.\n",
        "    - Suggestions to reduce computational time include specialized neural hardware and parallel computing techniques.\n"
      ],
      "metadata": {
        "id": "AOiiOsZc77AA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **5. Summary and Conclusions**\n",
        "\n",
        "#### **5.1 Overview of Non-Ontogenic Methods**\n",
        "- **Theoretical and Experimental Studies:**\n",
        "  - These methods examine the impact of partially connected networks (PCNNs) in terms of dynamics.\n",
        "  - Approaches include random unit selection, weight dilution, varying connectivity, and exhaustive methods.\n",
        "  - Results suggest there is an optimal connectivity level specific to each problem for improved performance.\n",
        "\n",
        "- **Biologically-Inspired PCNNs:**\n",
        "  - These models mimic the sparse connectivity of biological neural networks.\n",
        "  - Methods involve simulations of neural spike activities, statistical measures, and synaptic growth models.\n",
        "\n",
        "- **Application-Specific PCNNs:**\n",
        "  - Methods designed for specific domains like speech recognition and image processing.\n",
        "  - Connectivity is based on relationships within data, local connections, and geometrical patterns.\n",
        "\n",
        "#### **5.2 Overview of Ontogenic Methods**\n",
        "- **General Aim:**\n",
        "  - Ontogenic PCNNs focus on enhancing generalization by dynamically adjusting network topology during training.\n",
        "  - Techniques include pruning connections during training or after training to find the minimal effective topology.\n",
        "\n",
        "#### **5.3 Overview of Hybrid Methods**\n",
        "- **Knowledge-Based PCNNs:**\n",
        "  - These methods are still in the early stages but aim to combine symbolic domain knowledge and neural networks for effective network topology.\n",
        "  - Further developments may include automatic rule extraction.\n",
        "\n",
        "- **Genetic Programming-Based Methods:**\n",
        "  - These methods use genetic algorithms to optimize neural network topology and weights.\n",
        "  - Computational complexity is a major challenge, and suggestions for improvement include hardware optimization and parallel computing.\n"
      ],
      "metadata": {
        "id": "i74WaOtU77EH"
      }
    }
  ]
}