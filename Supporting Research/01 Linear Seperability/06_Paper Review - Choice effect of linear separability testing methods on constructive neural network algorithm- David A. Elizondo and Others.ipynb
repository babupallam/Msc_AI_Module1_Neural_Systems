{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true,"authorship_tag":"ABX9TyPzprlffo9msPxYAluPSGjD"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["## Title: **Choice effect of linear separability testing methods on constructive neural network algorithms: An empirical study**\n","\n","## Authors\n","- **David A. Elizondo** - Centre for Computational Intelligence, School of Computing, Faculty of Computing Sciences and Engineering, De Montfort University, Leicester, UK\n","- **J.M. Ortiz-de-Lazcano-Lobato** - Department of Computer Science and Artificial Intelligence, University of Málaga, Spain\n","- **Ralph Birkenhead** - Centre for Computational Intelligence, School of Computing, Faculty of Computing Sciences and Engineering, De Montfort University, Leicester, UK\n","\n","## Aim of the Paper\n","The paper aims to study the impact of different methods for testing linear separability on the performance of constructive neural network algorithms. The focus is on how the choice of testing algorithm affects the neural network's topology size, convergence time, and generalization level.\n","\n","## Key Proposals\n","- Six different methods for testing linear separability were used in the study, which includes:\n","  - Four exact methods: Simplex, Convex Hull, Support Vector Machine (SVM), and Perceptron.\n","  - Two approximative methods: Anderson and Fisher.\n","- The study investigates how these methods influence the structure and learning behavior of Recursive Deterministic Perceptron (RDP) neural networks.\n","\n","## Benchmarks\n","The study uses nine machine learning benchmarks, including:\n","- Iris\n","- Soybean\n","- Glass\n","- Hepatitis\n","- Wine\n","- Ionosphere\n","- Wisconsin Breast Cancer\n","- Sonar\n","\n","## Conclusion\n","The study concluded that the choice of linear separability testing method significantly affects the neural network's performance in terms of network size, speed of convergence, and generalization capability.\n"],"metadata":{"id":"rAlOgCJh6jnp"}},{"cell_type":"markdown","source":["# Detailed Review"],"metadata":{"id":"BbkxCn7g6jkG"}},{"cell_type":"markdown","source":["#### 1. Introduction\n","\n","- **Overview**\n","  - Several algorithms exist for testing linear separability.\n","  - These algorithms are critical in constructive neural network algorithms, which transform a non-linearly separable problem into a linearly separable one.\n","\n","- **Recursive Deterministic Perceptron (RDP)**\n","  - RDP is one such algorithm for separating two or more classes, even if they are not separable by a simple perceptron.\n","  - The main approach involves augmenting the affine dimension of the input vector by adding outputs from intermediate neurons as new components.\n","\n","- **Problem Statement**\n","  - The paper investigates how different linear separability testing methods impact the performance of RDP-based neural networks, specifically:\n","    - Topology size.\n","    - Convergence time.\n","    - Generalization level.\n","\n","#### 2. RDP Construction Methods\n","\n","- **Batch, Incremental, and Modular Learning**\n","  - Three methods are highlighted for constructing RDP neural networks:\n","    - **Batch Learning**:\n","      - This method is limited by its NP-Complete strategy, though it produces small topologies.\n","    - **Incremental Learning**:\n","      - The method selected for this study.\n","      - Offers polynomial time complexity but results in slightly larger topologies.\n","    - **Modular Learning**:\n","      - Also has polynomial time complexity, similar to the Incremental method.\n","\n","#### 3. Aim of the Study\n","\n","- **Objective**\n","  - The study aims to empirically measure the effects of various linear separability testing methods on:\n","    - Network size.\n","    - Speed of convergence.\n","    - Generalization capabilities.\n","\n","#### 4. Machine Learning Benchmarks\n","\n","- **Datasets Used**\n","  - Nine machine learning benchmarks were employed for performance comparisons:\n","    - Iris\n","    - Soybean\n","    - MONK's Problem 3\n","    - Glass\n","    - Hepatitis\n","    - Wine\n","    - Ionosphere\n","    - Wisconsin Breast Cancer\n","    - Sonar\n","\n","- **Algorithms Compared**\n","  - The RDP method was compared with backpropagation and Cascade Correlation algorithms for completeness.\n","`"],"metadata":{"id":"RaOPeqoI6jhh"}},{"cell_type":"markdown","source":["### 2. Methods for Testing Linear Separability\n","\n","- **Definition of Linear Separability**\n","  - Two subsets \\( X \\) and \\( Y \\) in \\( R^d \\) are linearly separable (LS) if there exists a hyperplane that separates all elements of \\( X \\) from \\( Y \\).\n","\n","- **Overview of Methods**\n","  - Six methods are used to test linear separability, divided into:\n","    - **Exact Methods**\n","    - **Approximative Methods**\n","\n","#### 2.1 Exact Methods for Testing Linear Separability\n","\n","- **Simplex Algorithm**\n","  - Represents the classification problem as a set of constrained linear equations.\n","  - If the two classes are LS, the Simplex algorithm finds a solution to these equations.\n","  - **Output**: Always guarantees a solution for linearly separable problems.\n","\n","- **Convex Hull Algorithm**\n","  - If two classes are LS, the intersection of their convex hulls will be empty.\n","  - Uses Kozinec's algorithm to generate vectors that converge to a final separating hyperplane.\n","  - **Output**: Converges after a finite number of steps.\n","\n","- **Perceptron Algorithm**\n","  - This neural network learning algorithm ensures convergence if the two classes are LS.\n","  - **Output**: Guarantees finding a separating hyperplane within a finite number of steps.\n","\n","- **Support Vector Machine (SVM)**\n","  - Uses quadratic programming to solve for the hyperplane that separates two classes.\n","  - **Output**: Finds the optimal separating hyperplane for LS problems by solving an optimization problem.\n","\n","#### 2.2 Approximative Methods for Testing Linear Separability\n","\n","- **Anderson's Method**\n","  - Assumes that the points in each class follow a multivariate Gaussian distribution.\n","  - Aims to minimize the classification error by finding a hyperplane that separates the datasets with minimal misclassification.\n","\n","- **Fisher Linear Discriminant**\n","  - Finds a linear combination of input variables that maximizes the separation between class projections while minimizing the within-class variance.\n","  - **Output**: Provides a separation based on the statistical properties of the classes, without assuming any specific probability distribution.\n"],"metadata":{"id":"dNfDC6tA6je2"}},{"cell_type":"markdown","source":["### 3. The Incremental Recursive Deterministic Perceptron (RDP)\n","\n","- **Overview**\n","  - The Recursive Deterministic Perceptron (RDP) neural network is a constructive neural network designed to address two-class classification problems, even when the classes are non-linearly separable (NLS).\n","  - The RDP's construction is based on adding intermediate neurons and hyperplanes that iteratively separate linearly separable subsets from the non-linearly separable dataset.\n","  - The neural network grows incrementally as new data points are added.\n","\n","#### 3.1 Incremental Learning Approach\n","\n","- **Description of Method**\n","  - In incremental or progressive learning, the RDP network is trained using a subset of the training dataset.\n","  - Training begins by classifying the points from the class of maximum cardinality and continues with the remaining data points one at a time.\n","  - Each point is passed through the network, and if it is not classified correctly, a new intermediate neuron (IN) is added.\n","  - This process continues until all data points are correctly classified, ensuring that new knowledge is interpolated without disturbing the previously learned information.\n","  \n","- **Illustrative Example**\n","  - The incremental learning method is demonstrated using a two-dimensional classification problem.\n","  - A small subset of points is selected from the two classes, and the method iteratively builds the RDP network by adding new intermediate neurons as needed until all points are classified correctly.\n","\n","#### 3.2 Key Characteristics of RDP\n","\n","- **Guaranteed Convergence**\n","  - The RDP is guaranteed to converge, meaning it will always find a solution even for non-linearly separable datasets.\n","  \n","- **Incremental Learning**\n","  - New data points are incorporated into the model without re-training from scratch, making the approach efficient for real-time applications.\n","\n","- **Comparison to Other Algorithms**\n","  - The RDP is compared to other neural network models such as backpropagation and Cascade Correlation algorithms.\n","  - Unlike backpropagation, the RDP does not require parameter tuning or suffer from issues like catastrophic interference.\n","  - The RDP's generalization capabilities are comparable to those of Cascade Correlation, which is also an incremental learning algorithm.\n"],"metadata":{"id":"q4Gtf4yo6jb9"}},{"cell_type":"markdown","source":["### 4. Performance Comparison Procedure\n","\n","- **Overview**\n","  - The performance comparison procedure involved using nine machine learning benchmark datasets.\n","  - The aim was to compare the performance of different methods used for testing linear separability while building Recursive Deterministic Perceptron (RDP) neural networks.\n","\n","#### 4.1 Benchmark Datasets\n","\n","- **Iris Dataset**\n","  - A well-known dataset used for classification of plants.\n","  - Only two classes, Iris Versicolour and Iris Virginica, were used in the study, as Iris Setosa is linearly separable from the other two.\n","\n","- **Soybean Dataset**\n","  - Used for diagnosing diseases in the Soybean crop.\n","  - The study focused on three classes: brown-spot, alternaria leaf-spot, and frog-eye-leaf-spot.\n","\n","- **Wisconsin Breast Cancer Dataset**\n","  - A binary classification problem to distinguish between benign and malignant breast cancer cases.\n","  - 699 instances and nine attributes were used for the classification.\n","\n","- **Glass Dataset**\n","  - Contains 163 instances of four types of window glass and non-window glass, used in criminological investigations.\n","\n","- **Hepatitis Dataset**\n","  - The dataset contains discrete and continuous attributes related to people with hepatitis, classified based on whether they healed or not.\n","\n","- **Ionosphere Dataset**\n","  - Collected from radar data for classifying radar returns as \"good\" or \"bad.\"\n","\n","- **Wine Dataset**\n","  - A chemical analysis of three types of wine, focusing on binary classification between two of the classes.\n","\n","- **Sonar Dataset**\n","  - Classification of sonar targets as rocks or mines using frequency-based inputs.\n","\n","- **MONK’s 3 Dataset**\n","  - A noisy dataset used for classification, selected for its challenging nature due to input pattern noise.\n","\n","#### 4.2 Performance Metrics\n","\n","- **Convergence Time**\n","  - The time taken by each RDP neural network model to converge.\n","  - Results were recorded for the different linear separability testing methods.\n","\n","- **Topology Size**\n","  - The number of intermediate neurons required to transform the non-linearly separable problem into a linearly separable one.\n","  - Smaller topology size indicates a more efficient neural network model.\n","\n","- **Generalization Capability**\n","  - How well the neural network generalized to previously unseen data.\n","  - Results were measured using testing datasets.\n","\n","#### 4.3 Cross-Validation\n","\n","- **Training and Testing Datasets**\n","  - The datasets were split into training and testing sets using cross-validation techniques.\n","  - For each benchmark, 10 different neural networks were developed and tested to evaluate performance stability.\n"],"metadata":{"id":"o5QFfB6B6jZj"}},{"cell_type":"markdown","source":["### 5. Results\n","\n","- **Overview**\n","  - This section presents the performance results of various methods for testing linear separability (LS) using the Recursive Deterministic Perceptron (RDP) models across different benchmarks.\n","  - The performance is measured by three key factors:\n","    - **Convergence time**\n","    - **Generalization ability**\n","    - **Topology size (number of intermediate neurons)**\n","\n","#### 5.1 Convergence Time\n","\n","- **Exact Methods**\n","  - **Simplex Algorithm**\n","    - The fastest in most datasets with the smallest topology size.\n","    - For instance, on the **Glass dataset**, the Simplex algorithm had an average time of **3.47 seconds**.\n","  - **Support Vector Machine (SVM)**\n","    - Performed comparably to Simplex in some cases but typically slower.\n","    - Required fine-tuning of parameters to achieve acceptable convergence.\n","  - **Convex Hull**\n","    - A mid-tier performer in terms of time.\n","  \n","- **Approximative Methods**\n","  - **Anderson Method**\n","    - In most cases, Anderson performed better than Fisher in terms of convergence speed.\n","    - On average, it outperformed Fisher by around **10-15%** in most datasets.\n","  - **Fisher Discriminant**\n","    - Slower in comparison to Anderson, showing higher variability in convergence times.\n","\n","#### 5.2 Generalization Capability\n","\n","- **Exact Methods**\n","  - There was **no clear overall winner** in terms of generalization across all benchmarks.\n","  - The Simplex method and Perceptron performed comparably, but each had instances of poor performance in specific datasets.\n","  - **SVM**, despite its good performance in convergence, did not outperform other methods in generalization on any dataset.\n","  \n","- **Approximative Methods**\n","  - **Anderson** was generally superior to **Fisher** in terms of generalization, but both had lower generalization levels compared to the exact methods.\n","  - The generalization capacity was generally lower with approximative methods due to their reliance on approximating LS.\n","\n","#### 5.3 Topology Size\n","\n","- **Exact Methods**\n","  - The **Simplex method** consistently produced the smallest topologies, which implies a more compact and efficient neural network model.\n","  - **SVM** produced similar topology sizes but was generally slower in terms of time.\n","  - **Convex Hull** and **Perceptron** methods typically produced slightly larger topologies.\n","\n","- **Approximative Methods**\n","  - Anderson again performed better than Fisher, producing smaller topologies on average.\n","  - However, both approximative methods produced larger topologies compared to the exact methods.\n","\n","#### 5.4 Summary of Results\n","\n","- **Exact Methods**\n","  - **Simplex** was the overall best performer, particularly in terms of speed and topology size.\n","  - **SVM** and **Convex Hull** had comparable performance in specific cases but were generally outpaced by Simplex.\n","  \n","- **Approximative Methods**\n","  - **Anderson** outperformed **Fisher** in all metrics (speed, generalization, and topology size), but neither matched the performance of the exact methods.\n"],"metadata":{"id":"88xi76e_6jWx"}},{"cell_type":"markdown","source":["### 6. Conclusions\n","\n","- **Overview**\n","  - The paper provides a comprehensive empirical study on the impact of different linear separability (LS) testing methods on the Recursive Deterministic Perceptron (RDP) neural networks.\n","  - Several methods, both exact and approximative, were compared in terms of:\n","    - Convergence time\n","    - Topology size\n","    - Generalization ability\n","\n","#### 6.1 Key Findings\n","\n","- **Exact Methods**\n","  - The **Simplex method** generally performed best in terms of convergence time and topology size.\n","  - Other exact methods, such as **SVM** and **Convex Hull**, were also effective but less consistent in their performance compared to Simplex.\n","  - The generalization levels between exact methods showed no clear overall winner, although all methods maintained good generalization across most datasets.\n","  \n","- **Approximative Methods**\n","  - **Anderson's method** consistently outperformed **Fisher's method** in convergence time, generalization capability, and topology size.\n","  - However, these methods did not match the generalization or efficiency of the exact methods, as approximative methods typically produced larger topologies and took longer to converge.\n","\n","#### 6.2 Impact on Neural Network Training\n","\n","- **Generalization**\n","  - The generalization capabilities of the RDP neural networks using exact methods, such as the Simplex, are comparable to more commonly used neural network algorithms like Backpropagation and Cascade Correlation.\n","  \n","- **Applicability**\n","  - The results confirm that the selection of an LS testing method is crucial for the efficiency of RDP-based neural networks, particularly in incremental learning scenarios where network topologies need to be kept compact and training time minimized.\n","\n","#### 6.3 Future Work\n","\n","- **Further Research**\n","  - Future studies should focus on refining the approximative methods for LS testing or developing hybrid approaches that could combine the speed of approximative methods with the accuracy of exact methods.\n","  - Exploration of other benchmark datasets or real-world applications could provide further insight into the adaptability of these methods for diverse neural network tasks.\n"],"metadata":{"id":"Ehd90M246jT7"}},{"cell_type":"markdown","source":[],"metadata":{"id":"B-RuHVWP6jRP"}},{"cell_type":"markdown","source":[],"metadata":{"id":"-ubl5h0v6jOq"}},{"cell_type":"markdown","source":[],"metadata":{"id":"tsjjpwgi6jLh"}},{"cell_type":"markdown","source":[],"metadata":{"id":"1S2Os8Hr6jIf"}},{"cell_type":"markdown","source":[],"metadata":{"id":"lHPwa0N76jF7"}},{"cell_type":"markdown","source":[],"metadata":{"id":"gYVQRcbQ6jDR"}},{"cell_type":"markdown","source":[],"metadata":{"id":"D8DGCLHD6jAX"}},{"cell_type":"markdown","source":[],"metadata":{"id":"mhtbXVx-6i9h"}},{"cell_type":"markdown","source":[],"metadata":{"id":"ntyOsY8x6i6o"}},{"cell_type":"markdown","source":[],"metadata":{"id":"M1xe4vV66i3-"}},{"cell_type":"markdown","source":[],"metadata":{"id":"qZaBJJX96i1K"}},{"cell_type":"markdown","source":[],"metadata":{"id":"vd9pNmsY6iyg"}},{"cell_type":"markdown","source":[],"metadata":{"id":"pUK1w56I6ivt"}},{"cell_type":"markdown","source":[],"metadata":{"id":"CuKBInIm6itK"}},{"cell_type":"markdown","source":[],"metadata":{"id":"vlu-6hue6iqa"}},{"cell_type":"markdown","source":[],"metadata":{"id":"FlBfXS-u6inm"}}]}