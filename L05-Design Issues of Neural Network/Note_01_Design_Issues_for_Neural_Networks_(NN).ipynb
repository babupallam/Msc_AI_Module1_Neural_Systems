{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyND1cnUaLQjVeXbhi5KlZpk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/babupallam/Msc_AI_Module1_Neural_Systems/blob/main/L05-Design%20Issues%20of%20Neural%20Network/Note_01_Design_Issues_for_Neural_Networks_(NN).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **1. Topology and Connectivity of Neural Networks (NN)**\n",
        "\n"
      ],
      "metadata": {
        "id": "0mKgt-4mZ2jC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Overview of Topology**\n",
        "  - Topology refers to the arrangement of neurons and their interconnections within a neural network.\n",
        "  - It determines how the layers are organized and how information flows between them.\n",
        "  - A well-designed topology can greatly affect the network's ability to learn, generalize, and efficiently process data.\n",
        "\n"
      ],
      "metadata": {
        "id": "aA5-d_u2Z2gR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Types of Neural Network Connectivity**\n",
        "  "
      ],
      "metadata": {
        "id": "lVIZiJJlZ2db"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Full Connectivity (Default Configuration)**:\n",
        "\n",
        "  - Every neuron in one layer is connected to every neuron in the next layer.\n",
        "  - Provides a dense network that can model highly complex relationships.\n",
        "  - **Advantages**:\n",
        "    - Maximizes the network's capability to learn a wide variety of features.\n",
        "    - Best for tasks requiring high capacity, such as complex image recognition problems.\n",
        "  - **Disadvantages**:\n",
        "    - High redundancy in connections, leading to unnecessary complexity.\n",
        "    - Increased risk of overfitting, especially for small datasets.\n",
        "    - Requires more computational resources for training and inference.\n",
        "\n"
      ],
      "metadata": {
        "id": "73sd6LoEfmdY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Partial Connectivity**:\n",
        "  - Only a subset of connections exists between neurons in consecutive layers.\n",
        "  - Inspired by biological neural systems, where not all neurons are fully connected.\n",
        "  - **Advantages**:\n",
        "    - **Reduced Training Time**: Fewer connections result in fewer weights to optimize, speeding up training.\n",
        "    - **Improved Generalization**: Reducing the number of parameters can help prevent overfitting.\n",
        "    - **Lower Hardware Requirements**: Reduced complexity leads to lower memory and processing needs.\n",
        "    - **Closer to Biological Reality**: Mimics the partial connectivity seen in biological brains, adding to the network's efficiency.\n",
        "  - **Disadvantages**:\n",
        "    - May reduce the capacity of the network to capture complex relationships in the data.\n",
        "    - Requires careful design to ensure optimal performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "j1TrnzKCZ2aV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Topological Structures of Neural Networks**\n",
        "  "
      ],
      "metadata": {
        "id": "0dlQ6Wa7Z2Xv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fully Connected Neural Network**:\n",
        "  - Each neuron in a layer is connected to every neuron in the next layer.\n",
        "  - Most common type for feedforward neural networks.\n",
        "  - **MATLAB Example**:\n",
        "    ```matlab\n",
        "    % Define a fully connected neural network\n",
        "    layers = [\n",
        "        sequenceInputLayer(10)\n",
        "        fullyConnectedLayer(50)\n",
        "        fullyConnectedLayer(20)\n",
        "        fullyConnectedLayer(1)\n",
        "        regressionLayer];\n",
        "    \n",
        "    % Create and train the network\n",
        "    options = trainingOptions('adam', 'MaxEpochs', 100);\n",
        "    net = trainNetwork(inputData, targetData, layers, options);\n",
        "    ```\n",
        "    In this example, the network consists of three fully connected layers where each neuron in the current layer is connected to all neurons in the following layer.\n",
        "\n"
      ],
      "metadata": {
        "id": "f53GinOrfzXG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Plenary Neural Network**:\n",
        "  - Contains all possible interlayer, intralayer, supralayer, and self-connections.\n",
        "  - Suitable for associative memories, which require high connectivity.\n",
        "  - **Plenary Without Self-Connections**: Used in networks where associative memory is required but self-connections would interfere with functionality.\n",
        "  \n"
      ],
      "metadata": {
        "id": "RK0UCfvbZ2Ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fully Interlayer Connected Neural Network**:\n",
        "  - Contains connections only between layers, avoiding any intra-layer connections.\n",
        "  - Used to simplify the model by avoiding unnecessary connections.\n"
      ],
      "metadata": {
        "id": "Bp33mYixgAd-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ### **Partial Connectivity Example in MATLAB**:\n",
        "  - Partial connectivity can be simulated using custom weight masks or by defining a custom layer that only allows certain connections.\n",
        "  - **MATLAB Example**:\n",
        "    ```matlab\n",
        "    % Define a partially connected neural network using a custom layer\n",
        "    layers = [\n",
        "        sequenceInputLayer(10)\n",
        "        fullyConnectedLayer(50)\n",
        "        customPartialConnectedLayer(20) % Custom layer for partial connectivity\n",
        "        fullyConnectedLayer(1)\n",
        "        regressionLayer];\n",
        "    \n",
        "    % Custom Layer Definition\n",
        "    classdef customPartialConnectedLayer < nnet.layer.Layer\n",
        "        properties\n",
        "            % Define the properties for partial connection\n",
        "            NumNeurons\n",
        "        end\n",
        "        \n",
        "        methods\n",
        "            function layer = customPartialConnectedLayer(numNeurons, name)\n",
        "                % Create a partially connected layer\n",
        "                layer.Name = name;\n",
        "                layer.NumNeurons = numNeurons;\n",
        "            end\n",
        "            \n",
        "            function Z = predict(layer, X)\n",
        "                % Define the forward pass with partial connectivity\n",
        "                % Only a subset of neurons are connected\n",
        "                mask = rand(size(X, 2), layer.NumNeurons) > 0.5; % 50% connectivity mask\n",
        "                Z = X * mask;\n",
        "            end\n",
        "        end\n",
        "    end\n",
        "    \n",
        "    % Create and train the network\n",
        "    options = trainingOptions('adam', 'MaxEpochs', 100);\n",
        "    net = trainNetwork(inputData, targetData, layers, options);\n",
        "    ```\n",
        "    In this example, the custom layer (`customPartialConnectedLayer`) implements partial connectivity by using a random mask that controls which neurons are connected, simulating a sparse connection structure.\n",
        "\n"
      ],
      "metadata": {
        "id": "zGIoTjFJZ2Rx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Benefits and Drawbacks of Full vs. Partial Connectivity**\n",
        "  - **Fully Connected Networks**:\n",
        "    - **Benefits**:\n",
        "      - High capacity for feature extraction.\n",
        "      - Suitable for applications where abundant data is available to avoid overfitting.\n",
        "    - **Drawbacks**:\n",
        "      - Large number of parameters, leading to high computational costs.\n",
        "      - Prone to overfitting if data is insufficient or not diverse enough.\n",
        "  - **Partially Connected Networks**:\n",
        "    - **Benefits**:\n",
        "      - Lower number of parameters reduces the risk of overfitting.\n",
        "      - Faster convergence during training due to fewer weights.\n",
        "      - Can lead to improved generalization when the data size is limited.\n",
        "    - **Drawbacks**:\n",
        "      - May fail to capture all relevant features if connectivity is too sparse.\n",
        "      - Requires careful tuning to achieve the right balance of connections.\n",
        "\n"
      ],
      "metadata": {
        "id": "lPEUJLUmZ2PA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Topology Design Considerations**\n",
        "  - **Biological Inspiration**:\n",
        "    - Many NN topologies draw inspiration from the human brain, which relies on sparse connectivity for efficiency.\n",
        "    - Biological brains demonstrate that selective, sparse connections can still facilitate complex processing.\n",
        "  - **Overfitting Prevention**:\n",
        "    - Fully connected networks tend to overfit, especially on small datasets, due to a large number of parameters.\n",
        "    - Partially connected networks are often preferred for their ability to generalize better with limited data.\n",
        "  - **Hardware and Computational Efficiency**:\n",
        "    - Full connectivity demands extensive computational resources for storing and updating large weight matrices.\n",
        "    - For edge devices or hardware-limited scenarios, partial connectivity is more feasible.\n",
        "\n"
      ],
      "metadata": {
        "id": "QWyKoSn_eKMq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Applications of Different Topologies**\n",
        "  - **Full Connectivity**:\n",
        "    - Used in tasks like image recognition and language translation where model complexity is required.\n",
        "    - Ideal when training data is large and diverse, allowing the network to avoid overfitting.\n",
        "  - **Partial Connectivity**:\n",
        "    - Commonly seen in convolutional neural networks (CNNs) where neurons are connected only to a local region of the previous layer.\n",
        "    - Suitable for tasks like signal processing and object detection, where localized features are important.\n",
        "\n"
      ],
      "metadata": {
        "id": "FkY67FC9eJjy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- **Practical Challenges in Designing Topology**\n",
        "  - **Choosing Optimal Connectivity**:\n",
        "    - Finding the right balance between fully connected and partially connected topologies is crucial.\n",
        "    - Over-designing a network (adding too many connections) can lead to poor generalization, while under-designing can limit learning capacity.\n",
        "  - **Adaptability During Training**:\n",
        "    - Topologies may be modified during training using ontogenic methods (e.g., adding or pruning connections) to ensure the network adapts optimally to the data.\n"
      ],
      "metadata": {
        "id": "1ixQ-Lg-eM8a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Methods for Adding/Deleting Connections in Neural Networks**\n",
        "\n"
      ],
      "metadata": {
        "id": "etVSBvWAZ2MS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Overview of Methods**\n",
        "  - Methods for modifying connections in neural networks help optimize the network's performance by adjusting its structure.\n",
        "  - These methods are categorized as Ontogenic (dynamic), Non-Ontogenic (static), or Hybrid methods.\n",
        "  - The goal is to find the best possible network architecture that balances computational efficiency with predictive accuracy.\n",
        "\n"
      ],
      "metadata": {
        "id": "22KC6IqeZ2Jh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Ontogenic Methods (Dynamic Methods)**\n"
      ],
      "metadata": {
        "id": "-CXDmCyXZ2Gi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "  - **Definition**:\n",
        "    - Ontogenic methods dynamically modify the network structure during the learning phase.\n",
        "    - These methods help the network adaptively grow or prune connections to improve efficiency and accuracy.\n"
      ],
      "metadata": {
        "id": "UoWp1ByuZ2Dx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Types of Ontogenic Methods**:\n",
        "  "
      ],
      "metadata": {
        "id": "X7h-fWh2Z2A8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Connection Pruning**:\n",
        "  - Initially train the neural network with a larger-than-required topology.\n",
        "  - Gradually prune the network by removing unnecessary connections to reduce redundancy.\n",
        "  - **Advantages**:\n",
        "    - Reduces model complexity.\n",
        "    - Prevents overfitting by eliminating excessive parameters.\n",
        "    - Saves computational resources.\n",
        "  - **MATLAB Example**:\n",
        "    ```matlab\n",
        "    % Define a neural network with an initial large topology\n",
        "    layers = [\n",
        "        sequenceInputLayer(10)\n",
        "        fullyConnectedLayer(100) % Start with a large number of neurons\n",
        "        fullyConnectedLayer(50)\n",
        "        fullyConnectedLayer(1)\n",
        "        regressionLayer];\n",
        "    \n",
        "    % Train the network\n",
        "    options = trainingOptions('adam', 'MaxEpochs', 50);\n",
        "    net = trainNetwork(inputData, targetData, layers, options);\n",
        "    \n",
        "    % Prune connections by analyzing the weight matrices\n",
        "    for i = 1:length(net.Layers)\n",
        "        if isa(net.Layers{i}, 'nnet.cnn.layer.FullyConnectedLayer')\n",
        "            % Prune connections with very small weights\n",
        "            net.Layers{i}.Weights(abs(net.Layers{i}.Weights) < 0.01) = 0;\n",
        "        end\n",
        "    end\n",
        "    ```\n",
        "    In this example, a network is trained initially with an oversized architecture, and weights with small magnitudes are pruned to reduce redundancy.\n",
        "\n"
      ],
      "metadata": {
        "id": "Be9z9lGYgYtN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Growing Methods**:\n",
        "  - Start with a minimal topology, then incrementally add new connections as needed.\n",
        "  - Grow the network until the desired performance level is achieved.\n",
        "  - **Advantages**:\n",
        "    - Optimizes resource usage by adding complexity only when required.\n",
        "    - Ensures the network remains simple and computationally efficient.\n",
        "  - **MATLAB Example**:\n",
        "    ```matlab\n",
        "    % Define an initial small neural network\n",
        "    layers = [\n",
        "        sequenceInputLayer(10)\n",
        "        fullyConnectedLayer(5) % Start with a small number of neurons\n",
        "        fullyConnectedLayer(1)\n",
        "        regressionLayer];\n",
        "    \n",
        "    % Train the network\n",
        "    options = trainingOptions('adam', 'MaxEpochs', 50);\n",
        "    net = trainNetwork(inputData, targetData, layers, options);\n",
        "    \n",
        "    % Gradually grow the network\n",
        "    if validationAccuracy < targetAccuracy\n",
        "        % Add more neurons to improve performance\n",
        "        layers = [\n",
        "            sequenceInputLayer(10)\n",
        "            fullyConnectedLayer(10) % Increased number of neurons\n",
        "            fullyConnectedLayer(1)\n",
        "            regressionLayer];\n",
        "    end\n",
        "    ```\n",
        "    In this example, the network starts small, and additional neurons are added to improve performance when necessary.\n",
        "\n"
      ],
      "metadata": {
        "id": "q9d0WZODZ1-J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Non-Ontogenic Methods (Static Methods)**\n",
        "  - **Definition**:\n",
        "    - These methods determine the network's structure before training and do not modify the topology during the learning process.\n",
        "    - The connectivity pattern is fixed throughout training.\n",
        "  - **Categories of Non-Ontogenic Methods**:\n",
        "    - **Based on Theoretical Studies**:\n",
        "      - Structures are defined based on prior theoretical understanding and experience.\n",
        "    - **Biologically Inspired**:\n",
        "      - Networks are designed based on structures observed in biological neural networks.\n",
        "    - **Application-Dependent**:\n",
        "      - Networks are tailored to the specific needs of the problem at hand.\n",
        "    - **Modularity-Based**:\n",
        "      - Networks are divided into modules, each handling a different aspect of the input data.\n",
        "    - **Hardware-Based**:\n",
        "      - Topology designed to be efficiently implementable on specific hardware platforms, such as FPGAs or GPUs.\n",
        "  - **Advantages and Disadvantages**:\n",
        "    - **Advantages**:\n",
        "      - Simplifies the design process by fixing the network structure beforehand.\n",
        "      - Reduced complexity during training since topology is fixed.\n",
        "    - **Disadvantages**:\n",
        "      - Lacks adaptability; may not be optimal for all datasets.\n",
        "      - Can be inefficient if the initial design does not perfectly match the data's characteristics.\n",
        "\n"
      ],
      "metadata": {
        "id": "en7tttFsZ17j"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Hybrid Methods**\n"
      ],
      "metadata": {
        "id": "qCgCfd8teZEV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Definition**:\n",
        "  - Combine elements of both ontogenic and non-ontogenic methods to leverage the advantages of both.\n",
        "  - Use fixed structures with adaptive elements or combine neural networks with other AI techniques.\n"
      ],
      "metadata": {
        "id": "0WJPY79mg3fy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Examples of Hybrid Methods**:\n",
        "  - **Knowledge-Based Neural Networks**:\n",
        "    - Use symbolic knowledge in conjunction with neural networks to guide the topology selection.\n",
        "  - **Genetic Algorithm (GA) Based Optimization**:\n",
        "    - Use GA to find the optimal topology and weight values.\n",
        "    - **MATLAB Example**:\n",
        "      ```matlab\n",
        "      % Define an initial network structure\n",
        "      layers = [\n",
        "          sequenceInputLayer(10)\n",
        "          fullyConnectedLayer(20)\n",
        "          fullyConnectedLayer(1)\n",
        "          regressionLayer];\n",
        "      \n",
        "      % Objective: Minimize validation error by optimizing topology\n",
        "      % Genetic algorithm setup\n",
        "      fitnessFunction = @(topology) trainAndEvaluateNN(topology, inputData, targetData);\n",
        "      \n",
        "      % Run GA to find optimal topology\n",
        "      options = optimoptions('ga', 'MaxGenerations', 20);\n",
        "      [optimalTopology, fval] = ga(fitnessFunction, numberOfVariables, [], [], [], [], lb, ub, [], options);\n",
        "      ```\n",
        "      In this example, a genetic algorithm is used to optimize the network topology, adjusting the number of neurons and layers to minimize the validation error.\n",
        "\n"
      ],
      "metadata": {
        "id": "79WR5Lkog2_z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key Considerations When Adding/Deleting Connections**\n",
        "  - **Balance Between Complexity and Performance**:\n",
        "    - Adding too many connections can lead to overfitting, while too few can result in underfitting.\n",
        "    - Ontogenic methods help dynamically adjust complexity based on the training data.\n",
        "  - **Resource Management**:\n",
        "    - Dynamic methods like pruning help save computational resources by removing unnecessary weights.\n",
        "    - Growing methods ensure that resources are used optimally, only adding complexity when needed.\n",
        "  - **Application Requirements**:\n",
        "    - Application-specific networks can benefit from hybrid approaches that combine adaptability with prior knowledge.\n",
        "\n"
      ],
      "metadata": {
        "id": "YlG8RrVneatS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Design of the Training Set for Neural Networks**\n",
        "\n"
      ],
      "metadata": {
        "id": "k1jyxm3mZ1zO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Importance of Training Set Design**\n",
        "  - The training set plays a crucial role in determining the performance of the neural network (NN).\n",
        "  - Proper design ensures effective learning, good generalization, and robustness to unseen data.\n",
        "  - Incorrectly designed training sets can lead to overfitting, underfitting, or a biased model.\n",
        "\n"
      ],
      "metadata": {
        "id": "nYTS4DSUZ1wk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Number of Samples in the Training Set**\n"
      ],
      "metadata": {
        "id": "ommlATAAZ1t0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Representativeness of Classes**:\n",
        "  - Every class in the problem domain must be well represented in the training set.\n",
        "  - Imbalanced classes can lead to biased training, where the NN is less effective at recognizing underrepresented classes.\n"
      ],
      "metadata": {
        "id": "XRGmNNPXZ1rS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Subgroup Representation**:\n",
        "  - Training data should include several subgroups, each representing distinct patterns within the class.\n",
        "  - Ensures the network learns the diversity within each class and does not generalize poorly to unrepresented subgroups.\n"
      ],
      "metadata": {
        "id": "NOrMvEg-hdTS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Rule of Thumb for Training Set Size**:\n",
        "  - The number of training samples should be at least double the number of weights (parameters) in the network.\n",
        "  - Large neural networks require large training sets to avoid overfitting.\n"
      ],
      "metadata": {
        "id": "OnknbgdchdEp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **MATLAB Example**:\n",
        "  ```matlab\n",
        "  % Generate synthetic training data for two classes with balanced representation\n",
        "  numSamples = 500;\n",
        "  class1Data = randn(numSamples, 2) + [1, 1]; % Class 1 centered around (1,1)\n",
        "  class2Data = randn(numSamples, 2) - [1, 1]; % Class 2 centered around (-1,-1)\n",
        "  \n",
        "  % Combine data\n",
        "  inputData = [class1Data; class2Data];\n",
        "  targetData = [ones(numSamples, 1); -ones(numSamples, 1)]; % Labels: 1 for class 1, -1 for class 2\n",
        "  \n",
        "  % Define a simple neural network\n",
        "  layers = [\n",
        "      featureInputLayer(2)\n",
        "      fullyConnectedLayer(10)\n",
        "      fullyConnectedLayer(1)\n",
        "      regressionLayer];\n",
        "  \n",
        "  % Train the network\n",
        "  options = trainingOptions('adam', 'MaxEpochs', 100);\n",
        "  net = trainNetwork(inputData, targetData, layers, options);\n",
        "  ```\n",
        "  In this example, synthetic data for two classes is created with balanced representation to ensure equal learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "vFAYWrathcqG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training Set Diversity**\n"
      ],
      "metadata": {
        "id": "3HwHQs3CZ1oi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Statistical Variations**:\n",
        "  - The training set must capture all statistical variations within each class.\n",
        "  - This includes variations due to noise, slight changes in input conditions, and naturally occurring diversity.\n",
        "\n",
        "### **Representation of Real-World Conditions**:\n",
        "  - Training data should be representative of the conditions that the network will face during deployment.\n",
        "  - This helps ensure the robustness of the model.\n",
        "\n",
        "### **Subgroup Diversity**:\n",
        "  - Each subgroup should have a variety of samples to prevent the network from over-specializing on particular patterns.\n",
        "\n"
      ],
      "metadata": {
        "id": "myj8MFuGel6y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Input Data Preparation**\n"
      ],
      "metadata": {
        "id": "UtPRUdngelVW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Type of Variables**:\n",
        "  - Input variables can be of different types: Nominal, Ordinal, and Interval.\n",
        "  - It is crucial to encode them properly before feeding them to the NN.\n",
        "\n"
      ],
      "metadata": {
        "id": "IzhzU777h2Ob"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Nominal Variables**:\n",
        "  - These variables represent categorical data without numerical relationships (e.g., fruit type).\n",
        "  - **One-Hot Encoding** is commonly used to convert nominal variables into a binary format for neural networks.\n",
        "  - **MATLAB Example for One-Hot Encoding**:\n",
        "    ```matlab\n",
        "    % Nominal variable representing fruit type: Apple, Peach, Banana\n",
        "    fruitType = categorical({'Apple', 'Peach', 'Banana', 'Apple', 'Banana'});\n",
        "    % Convert to one-hot encoded form\n",
        "    encodedFruitType = onehotencode(fruitType, 1);\n",
        "    ```\n",
        "    This example shows how to convert nominal data into a format suitable for NN training.\n"
      ],
      "metadata": {
        "id": "ngP1jkxPh2By"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ordinal Variables**:\n",
        "  - Represent data with order but no fixed interval (e.g., quality grades A, B, C).\n",
        "  - Typically encoded using an ordinal scale or thermometer coding.\n",
        "  - **Thermometer Encoding**: Represents the value using a binary progression where higher values are more active.\n",
        "  - **MATLAB Example for Thermometer Encoding**:\n",
        "    ```matlab\n",
        "    % Ordinal variable representing egg category: A, B, C\n",
        "    eggCategory = {'A', 'B', 'C', 'A', 'C'};\n",
        "    % Define thermometer encoding (A > B > C)\n",
        "    thermometerEncoded = [1 1 1; 1 1 0; 1 0 0; 1 1 1; 1 0 0];\n",
        "    ```\n"
      ],
      "metadata": {
        "id": "Bh2oGl0bh10m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Interval Variables**:\n",
        "  - Represent continuous numerical data (e.g., temperature).\n",
        "  - Should be scaled appropriately to match the NN's activation limits.\n",
        "  - **Normalization** helps the model converge faster and prevents certain features from dominating due to different scales.\n",
        "  - **MATLAB Example for Normalizing Interval Variables**:\n",
        "    ```matlab\n",
        "    % Interval variable representing daily temperatures\n",
        "    temperatures = [20.0, 21.5, 19.4, 17.2, 23.0];\n",
        "    % Normalize to range [0, 1]\n",
        "    normTemperatures = (temperatures - min(temperatures)) / (max(temperatures) - min(temperatures));\n",
        "    ```\n",
        "    Normalizing data ensures that all features contribute equally to learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "YouFoJ8oh1Z2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Training Set Size and Complexity**\n",
        "\n",
        "### **Network Size Impact**:\n",
        "  - Larger networks need larger training sets to capture enough variation and prevent overfitting.\n",
        "  - For small networks, overfitting is less of a risk, but underfitting can occur if the training set is not diverse.\n",
        "\n",
        "### **Rule of Thumb for Training Size**:\n",
        "  - The minimum number of training samples should be at least twice the number of weights.\n",
        "  - More data is always beneficial, but computational cost must be balanced.\n",
        "\n",
        "### **Validation and Testing Data**:\n",
        "  - Training data should be separate from validation and testing data to ensure unbiased performance evaluation.\n",
        "\n"
      ],
      "metadata": {
        "id": "Sft05fceet9Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Input Data Scaling**\n",
        "  "
      ],
      "metadata": {
        "id": "pEEhL70FetjB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Scaling Methods**:\n",
        "  - **Min-Max Scaling**: Scales data to a fixed range, typically [0, 1] or [-1, 1].\n",
        "  - **Z-Score Normalization**: Converts the data to have zero mean and unit variance.\n",
        "\n",
        "### **Why Scaling is Necessary**:\n",
        "  - Equalizes the importance of variables to prevent large-valued inputs from dominating smaller ones.\n",
        "  - Helps the NN learn effectively by keeping weight updates within a small, predictable range.\n",
        "\n",
        "### **MATLAB Example for Z-Score Normalization**:\n",
        "  ```matlab\n",
        "  % Sample data\n",
        "  data = [10, 15, 20, 25, 30];\n",
        "  % Calculate mean and standard deviation\n",
        "  meanData = mean(data);\n",
        "  stdData = std(data);\n",
        "  % Z-score normalization\n",
        "  zScoreData = (data - meanData) / stdData;\n",
        "  ```\n",
        "  Z-score normalization ensures that data is centered and scaled, aiding effective learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "OoGI5WBuiKO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Scaling Inputs/Outputs in Neural Networks**\n",
        "\n"
      ],
      "metadata": {
        "id": "giV4D4xbZ1lw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Overview of Scaling**\n",
        "  - Scaling is an essential preprocessing step in neural network (NN) training.\n",
        "  - It ensures that all input and output features are within a suitable range to prevent one variable from dominating others.\n",
        "  - Proper scaling improves learning efficiency, convergence speed, and model stability.\n",
        "\n"
      ],
      "metadata": {
        "id": "s5c1OIXkZ1i6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Scaling Process**\n"
      ],
      "metadata": {
        "id": "0IZUjM1zZ1gH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Normalize Data to Match Activation Limits**:\n",
        "  - Neural networks have specific activation functions with defined limits, such as sigmoid ([0, 1]) or hyperbolic tangent ([-1, 1]).\n",
        "  - Scaling inputs and outputs ensures compatibility with these limits, leading to better performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "1eYz1C7SiylW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Common Scaling Methods**:\n",
        "  "
      ],
      "metadata": {
        "id": "0dXT-o3siyD1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Min-Max Scaling**:\n",
        "  - Rescales inputs to fit within a specified range, often [0, 1] or [-1, 1].\n",
        "  \n",
        "  \n",
        "  - **MATLAB Example**:\n",
        "    ```matlab\n",
        "    % Example data\n",
        "    data = [10, 15, 20, 25, 30];\n",
        "    % Define new scaling limits\n",
        "    newMin = 0;\n",
        "    newMax = 1;\n",
        "    % Calculate min and max of the data\n",
        "    dataMin = min(data);\n",
        "    dataMax = max(data);\n",
        "    % Apply min-max scaling\n",
        "    scaledData = ((data - dataMin) / (dataMax - dataMin)) * (newMax - newMin) + newMin;\n",
        "    ```\n",
        "    This example rescales the data to fit between [0, 1], which is suitable for many activation functions.\n",
        "\n"
      ],
      "metadata": {
        "id": "v91DnrWFi6SZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Z-Score Normalization (Standardization)**:\n",
        "  - Standardizes data based on mean (μ) and standard deviation (σ), resulting in data with zero mean and unit variance.\n",
        "\n",
        "\n",
        "  - Useful when data is distributed normally and the model benefits from zero-centered inputs.\n",
        "  - **MATLAB Example**:\n",
        "    ```matlab\n",
        "    % Example data\n",
        "    data = [10, 15, 20, 25, 30];\n",
        "    % Calculate mean and standard deviation\n",
        "    dataMean = mean(data);\n",
        "    dataStd = std(data);\n",
        "    % Apply z-score normalization\n",
        "    standardizedData = (data - dataMean) / dataStd;\n",
        "    ```\n",
        "    This example standardizes data to have a mean of 0 and a variance of 1, improving the efficiency of gradient-based optimization.\n",
        "\n"
      ],
      "metadata": {
        "id": "ra_nzMSci6A7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Advantages of Scaling**\n",
        "\n",
        "### **Equalization of Variable Importance**:\n",
        "  - Variables with different scales can have unequal impacts on the learning process.\n",
        "  - Scaling helps ensure that all features contribute equally to the network’s learning.\n",
        "\n",
        "### **Improved Learning Efficiency**:\n",
        "  - Prevents large-valued features from dominating smaller ones, which helps maintain efficient weight updates during training.\n",
        "  - Allows neural networks to converge more rapidly to optimal solutions by reducing gradient issues.\n",
        "\n",
        "### **Stabilization of Weight Updates**:\n",
        "  - Weights in a neural network are updated based on the input feature scale.\n",
        "  - Scaling keeps weights in a predictable range, avoiding problems such as exploding or vanishing gradients.\n",
        "\n",
        "### **Compatibility with Activation Functions**:\n",
        "  - Some activation functions, like sigmoid and tanh, perform optimally when inputs are within specific ranges (e.g., [-1, 1] or [0, 1]).\n",
        "  - Scaling inputs to match these ranges leads to more effective and faster learning.\n",
        "\n"
      ],
      "metadata": {
        "id": "n6rIG05DZ1dX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Scaling Techniques and Applications**\n",
        "  "
      ],
      "metadata": {
        "id": "L1crbMwaZ1ai"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Min-Max Scaling Applications**:\n",
        "  - Suitable for datasets that need to fit within activation function limits, such as sigmoid activation.\n",
        "  - Works well for images and other numerical data where the range is fixed.\n",
        "  - **MATLAB Code Example for Application**:\n",
        "    ```matlab\n",
        "    % Image data scaling example\n",
        "    imageData = randi([0, 255], 28, 28); % Example 28x28 pixel grayscale image\n",
        "    % Normalize pixel values between 0 and 1\n",
        "    scaledImageData = (imageData - 0) / (255 - 0);\n",
        "    ```\n",
        "    This code demonstrates the use of Min-Max scaling to normalize image pixel values between 0 and 1.\n"
      ],
      "metadata": {
        "id": "dQ27oLV_jW_f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Z-Score Normalization Applications**:\n",
        "  - Best for datasets with no specific upper or lower bounds, where preserving the distribution is important.\n",
        "  - Often used in financial and statistical applications where data values vary widely.\n",
        "  - **MATLAB Example for Time Series Data**:\n",
        "    ```matlab\n",
        "    % Time series data normalization\n",
        "    timeSeriesData = [100, 105, 110, 115, 120];\n",
        "    % Z-score normalization\n",
        "    normalizedTimeSeries = (timeSeriesData - mean(timeSeriesData)) / std(timeSeriesData);\n",
        "    ```\n",
        "    In this example, the time series data is normalized to have a zero mean and unit variance.\n",
        "\n"
      ],
      "metadata": {
        "id": "AaVemhSJjWg3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Inverse Scaling for Outputs**\n",
        "### **Importance of Inverse Scaling**:\n",
        "  - For models that produce scaled outputs, inverse scaling must be applied to interpret results in their original context.\n",
        "  - This is especially crucial when the outputs need to match real-world measurements (e.g., prices, temperatures).\n",
        "\n",
        "\n",
        "### **MATLAB Example for Inverse Scaling**:\n",
        "  ```matlab\n",
        "  % Original scaling parameters\n",
        "  originalMin = 0;\n",
        "  originalMax = 100;\n",
        "  % Scaled output value\n",
        "  scaledValue = 0.75;\n",
        "  % Inverse scaling to convert back to original range\n",
        "  originalValue = scaledValue * (originalMax - originalMin) + originalMin;\n",
        "  ```\n",
        "  This example demonstrates how to revert a scaled output back to its original value, preserving the interpretability of the model's predictions.\n",
        "\n"
      ],
      "metadata": {
        "id": "O34Bzwtje7gk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key Considerations for Scaling**\n",
        "  - **Choice of Scaling Technique**:\n",
        "    - The choice between Min-Max scaling and Z-score normalization depends on the dataset and activation function used.\n",
        "    - Min-Max scaling is preferred when input features have defined bounds or the network uses sigmoid activation.\n",
        "    - Z-score normalization is more suitable for data that follows a normal distribution without specific bounds.\n",
        "  - **Outliers**:\n",
        "    - Min-Max scaling can be sensitive to outliers, which may distort the scaling process.\n",
        "    - Z-score normalization is less sensitive to outliers but can still be influenced by extreme values.\n",
        "  - **Feature Engineering**:\n",
        "    - Proper scaling is a part of feature engineering, which greatly affects the quality of neural network predictions.\n",
        "    - Consistent scaling across training, validation, and test sets is critical for accurate model evaluation.\n",
        "\n"
      ],
      "metadata": {
        "id": "0gJyRocxe7Rp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Handling Outliers and Data Distribution in Neural Networks**\n",
        "\n"
      ],
      "metadata": {
        "id": "wO0xzpz1Z1YC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Overview**\n",
        "  - Proper handling of outliers and understanding data distribution are crucial for improving the performance of neural networks (NNs).\n",
        "  - Outliers can skew the learning process, while a well-distributed dataset ensures consistent training and generalization.\n",
        "\n"
      ],
      "metadata": {
        "id": "8-Ct1WKiZ1VL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Outliers**\n"
      ],
      "metadata": {
        "id": "UYqYgbSGZ1SW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Definition**:\n",
        "  - Outliers are data points that significantly differ from the majority of the dataset.\n",
        "  - They may arise from measurement errors, incorrect data entry, or natural variation in the data.\n",
        "\n"
      ],
      "metadata": {
        "id": "1_Z5h-5vj2Rm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Sources of Outliers**:\n",
        "  - **Measurement Errors**: Sensor malfunction or inaccurate readings can produce erroneous values.\n",
        "  - **Data Entry Errors**: Manual entry mistakes such as typos.\n",
        "  - **Genuine Outliers**: Valid but extreme data points that are inherent to the phenomenon being studied.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ojnNORa-j2Ni"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Impact on Neural Networks**:\n",
        "  - **Biasing the Model**: Outliers can disproportionately affect the training process, leading to a biased model.\n",
        "  - **Weight Update Problems**: Large errors caused by outliers can lead to instability during weight updates, especially in gradient-based optimization.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "8XJ-Q8APj2LG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Detection Methods**:\n",
        "  #### **Visual Inspection**: Use scatter plots, box plots, or histograms to visually identify outliers.\n",
        "  #### **Statistical Methods**: Use statistical measures like Z-score to detect values that fall outside the acceptable range.\n",
        "    - **Z-Score Method**: An absolute Z-score greater than 3 is often used to identify outliers.\n",
        "  - **MATLAB Example for Outlier Detection**:\n",
        "    ```matlab\n",
        "    % Sample data\n",
        "    data = [10, 12, 15, 18, 100, 19, 22];\n",
        "    % Calculate Z-score\n",
        "    zScores = (data - mean(data)) / std(data);\n",
        "    % Identify outliers (absolute Z-score > 3)\n",
        "    outliers = abs(zScores) > 3;\n",
        "    % Display outliers\n",
        "    disp('Outliers:');\n",
        "    disp(data(outliers));\n",
        "    ```\n",
        "    This MATLAB code calculates Z-scores for the dataset and identifies any values that qualify as outliers.\n"
      ],
      "metadata": {
        "id": "yppGB5PXj2IS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Handling Outliers**:\n",
        "  - **Removal**: Remove data points that are deemed outliers, especially if they result from errors.\n",
        "  - **Correction**: If possible, correct erroneous values (e.g., by using averages from similar data points).\n",
        "  - **Capping**: Replace extreme values with upper or lower percentile limits.\n",
        "  - **Replacement with Mean/Median**: Replace outliers with the mean or median value of the dataset to retain information without extreme values.\n",
        "  - **MATLAB Example for Handling Outliers**:\n",
        "    ```matlab\n",
        "    % Replace outliers with median value\n",
        "    medianValue = median(data);\n",
        "    data(outliers) = medianValue;\n",
        "    % Display modified data\n",
        "    disp('Data after handling outliers:');\n",
        "    disp(data);\n",
        "    ```\n",
        "    This code replaces outliers with the median value of the dataset to reduce their impact on training.\n",
        "\n"
      ],
      "metadata": {
        "id": "6Kx04N9hj2F2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Data Distribution**\n"
      ],
      "metadata": {
        "id": "12sjD7bWZ1P6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### **Importance of Balanced Data Distribution**:\n",
        "  - A well-distributed dataset ensures that the neural network learns evenly from all features.\n",
        "  - Uneven distribution can lead to bias, where the model becomes overly influenced by features with higher variance or frequent occurrence.\n",
        "\n"
      ],
      "metadata": {
        "id": "8p57H0wEj2DP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Key Characteristics of Good Data Distribution**:\n",
        "  - **Similar Variance Across Features**:\n",
        "    - All input features should ideally have similar variances to ensure that each feature contributes equally during training.\n",
        "    - Features with higher variance can dominate the learning process, leading to suboptimal results.\n",
        "  - **Symmetric Distribution**:\n",
        "    - The distribution of data should be approximately symmetric, avoiding heavy tails that can skew learning.\n",
        "    - Heavy-tailed distributions may result in larger gradients, causing instability during weight updates.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XcAvCZ5Yj2A4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Methods for Assessing Data Distribution**:\n",
        "- **Descriptive Statistics**: Calculate mean, variance, skewness, and kurtosis to assess the overall distribution.\n",
        "- **Visualization Tools**: Use histograms, box plots, or Q-Q plots to visually inspect the distribution.\n",
        "- **MATLAB Example for Visualizing Data Distribution**:\n",
        "  ```matlab\n",
        "  % Sample data\n",
        "  data = randn(1, 1000) * 5 + 10; % Normally distributed data\n",
        "  % Plot histogram to visualize distribution\n",
        "  histogram(data, 30);\n",
        "  title('Data Distribution');\n",
        "  xlabel('Value');\n",
        "  ylabel('Frequency');\n",
        "  ```\n",
        "  The MATLAB code above generates a histogram to help visualize the data distribution.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "E-z1OWRtj1-a"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Ensuring Balanced Distribution**:\n",
        "- **Normalization/Standardization**: Apply techniques like Min-Max scaling or Z-score normalization to standardize the distribution across features.\n",
        "- **Data Transformation**: Apply transformations such as logarithmic or Box-Cox transformations to reduce skewness and achieve a more symmetric distribution.\n",
        "  - **MATLAB Example for Data Transformation**:\n",
        "    ```matlab\n",
        "    % Sample data with skewness\n",
        "    data = [1, 2, 2, 3, 5, 8, 13, 21, 34, 55];\n",
        "    % Apply logarithmic transformation\n",
        "    transformedData = log(data + 1); % Adding 1 to avoid log(0)\n",
        "    % Display transformed data\n",
        "    disp('Transformed Data:');\n",
        "    disp(transformedData);\n",
        "    ```\n",
        "    This example demonstrates the use of logarithmic transformation to reduce skewness in the dataset.\n"
      ],
      "metadata": {
        "id": "DpyCHqFtj17o"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Small Random Noise for Generalization**\n",
        ""
      ],
      "metadata": {
        "id": "L96dpuRnfHW8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Benefits of Adding Noise**:\n",
        "  - Adding small random noise to the training data can improve the generalization capabilities of a neural network.\n",
        "  - It helps prevent overfitting by making the network less sensitive to small fluctuations in the training data.\n"
      ],
      "metadata": {
        "id": "oYhYUfovkZi1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gaussian Noise Addition**:\n",
        "  - Gaussian noise with a small standard deviation is commonly used for improving generalization.\n",
        "  - **MATLAB Example for Adding Noise**:\n",
        "    ```matlab\n",
        "    % Sample data\n",
        "    data = randn(1, 100) * 10;\n",
        "    % Add small Gaussian noise\n",
        "    noise = randn(size(data)) * 0.5; % Mean 0, small std deviation\n",
        "    noisyData = data + noise;\n",
        "    % Plot original vs noisy data\n",
        "    figure;\n",
        "    plot(data, 'b'); hold on;\n",
        "    plot(noisyData, 'r');\n",
        "    legend('Original Data', 'Noisy Data');\n",
        "    title('Effect of Adding Small Noise');\n",
        "    ```\n",
        "    In this example, small Gaussian noise is added to the data, and the impact is visualized.\n",
        "\n"
      ],
      "metadata": {
        "id": "mal72KkFkZCC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Training Neural Networks: Learning Rate, Momentum, Avoiding Local Minima, and Stopping Criteria**\n",
        "\n"
      ],
      "metadata": {
        "id": "MFgEAni4Z1NS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Learning Rate and Momentum**\n",
        "  "
      ],
      "metadata": {
        "id": "KV3tTUXUZ1Kj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Learning Rate (LR)**:\n",
        "\n",
        "  - It is a control knob for how fast or slow a machine learning model learns.\n",
        "    - When the model tries to learn from data, it adjusts its internal settings (called weights) to improve its predictions.\n",
        "    - The learning rate decides how big these adjustments should be.\n",
        "\n",
        "  - Commonly represented as α, it controls how much the weights are updated during training.\n",
        "  - **Adaptive Learning Rates**: Techniques like learning rate scheduling or adaptive optimizers (e.g., Adam, RMSprop) dynamically adjust the learning rate during training for improved efficiency.\n",
        "  - **MATLAB Example for Learning Rate**:\n",
        "    ```matlab\n",
        "    % Define neural network layers\n",
        "    layers = [\n",
        "        featureInputLayer(2)\n",
        "        fullyConnectedLayer(10)\n",
        "        reluLayer\n",
        "        fullyConnectedLayer(1)\n",
        "        regressionLayer];\n",
        "    \n",
        "    % Specify training options with learning rate\n",
        "    options = trainingOptions('sgdm', ...\n",
        "        'InitialLearnRate', 0.01, ...\n",
        "        'MaxEpochs', 100, ...\n",
        "        'MiniBatchSize', 10, ...\n",
        "        'LearnRateSchedule', 'piecewise', ...\n",
        "        'LearnRateDropFactor', 0.1, ...\n",
        "        'LearnRateDropPeriod', 30);\n",
        "    \n",
        "    % Train the network\n",
        "    net = trainNetwork(inputData, targetData, layers, options);\n",
        "    ```\n",
        "    In this example, the learning rate is initialized at 0.01 and is scheduled to drop by a factor of 0.1 every 30 epochs.\n",
        "\n"
      ],
      "metadata": {
        "id": "FPxKs3xtkiGe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### Observations:\n",
        "  - If the learning rate is set to 0:\n",
        "    -  the model won't learn at all. With a learning rate of 0, no changes are made to the weights, meaning the model will keep making the same predictions without any improvement, regardless of the data it's being trained on.\n",
        "\n",
        "  - **Learning Rate Too High:**\n",
        "    - **Instability**: If the learning rate is too high, the model may overshoot the optimal solution, causing erratic updates in weights and making it hard for the model to converge to a good solution.\n",
        "    - **Divergence**: In extreme cases, the loss (error) can increase instead of decrease, leading the model to diverge and never learn effectively.\n",
        "    - **Example**: Imagine trying to navigate to a destination by taking very large steps—you might keep overshooting the target and never get there.\n",
        "\n",
        "  - **Learning Rate Too Low:**\n",
        "    - **Slow Convergence**: If the learning rate is too low, the model will take very small steps toward the optimal solution, making the training process extremely slow.\n",
        "    - **Getting Stuck in Local Minima**: A low learning rate might make the model stuck in a local minimum, which is not the best solution but one that the model can't escape due to the small adjustments.\n",
        "    - **Wasted Resources**: Training may take a long time and consume more computational resources, without significant improvements.\n",
        "\n",
        "  - **Dynamic or Adaptive Learning Rates:**\n",
        "   - **Learning Rate Scheduling**: Techniques like learning rate decay or scheduling gradually reduce the learning rate during training. This allows the model to take larger steps initially and then fine-tune with smaller steps as it gets closer to the solution.\n",
        "   - **Adaptive Optimizers (e.g., Adam, RMSprop)**: These methods adjust the learning rate dynamically based on the progress of the training, which can lead to faster convergence and better performance. They can handle cases where different parameters in the model require different learning rates.\n",
        "\n",
        "- **Optimal Learning Rate Varies by Problem:**\n",
        "   - **Data and Model Complexity**: The right learning rate depends on the specific problem, data, and model architecture. Complex models or noisy datasets may require different tuning compared to simpler ones.\n",
        "   - **Empirical Tuning**: Finding the best learning rate often requires experimenting with different values or using methods like grid search or random search to find what works best.\n",
        "\n",
        "- **Warm Restarts:**\n",
        "   - In some cases, cyclical learning rates or learning rate warm restarts are used. This technique periodically increases and decreases the learning rate during training, which can help the model explore different regions of the solution space and avoid local minima.\n",
        "\n",
        "- **Batch Size Interaction:**\n",
        "   - The size of the mini-batches used in training can affect the optimal learning rate. Larger batch sizes may allow for higher learning rates, while smaller batches often benefit from lower rates.\n",
        "\n",
        "- **Momentum and Learning Rate:**\n",
        "   - Optimizers like stochastic gradient descent with momentum can help in smoothing the learning process when learning rates are high by accumulating previous gradients to make better decisions on updates. Momentum works well when combined with appropriate learning rate adjustments.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "HlQSU2znvYRv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Momentum**:\n",
        "  - Momentum is another hyperparameter used to accelerate the convergence of the gradient descent algorithm.\n",
        "  \n",
        "  - **Parameter Representation**: Typically represented as β, momentum smooths the gradient descent by preventing large oscillations in the learning trajectory.\n",
        "  - **MATLAB Example for Momentum**:\n",
        "    ```matlab\n",
        "    % Specify training options with momentum\n",
        "    options = trainingOptions('sgdm', ...\n",
        "        'InitialLearnRate', 0.01, ...\n",
        "        'Momentum', 0.9, ...\n",
        "        'MaxEpochs', 100, ...\n",
        "        'MiniBatchSize', 10);\n",
        "    \n",
        "    % Train the network\n",
        "    net = trainNetwork(inputData, targetData, layers, options);\n",
        "    ```\n",
        "    In this example, momentum is set to 0.9 to stabilize the weight updates and accelerate convergence.\n",
        "\n"
      ],
      "metadata": {
        "id": "akM2ViU3khun"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "###### Observation 1:\n",
        "\n",
        "- It helps the network avoid getting stuck in local minima by adding a fraction of the previous weight update to the current update.\n",
        "- Explanation:\n",
        "  - Gradient Descent Challenges:\n",
        "      In standard gradient descent, weight updates are made based solely on the current gradient, which can lead to challenges, especially when the model encounters local minima (a suboptimal point where the gradient is zero or close to zero). The algorithm may get \"stuck\" in such points and fail to explore further regions of the solution space.\n",
        "\n",
        "  - Role of Momentum:\n",
        "    Momentum addresses this by allowing the algorithm to maintain a memory of previous updates. Instead of relying purely on the current gradient (which may be close to zero in a local minimum), the momentum term incorporates a fraction of the previous weight updates into the current step. This cumulative effect allows the algorithm to \"build speed\" in directions that consistently reduce the loss function and avoid being stuck in small fluctuations or shallow local minima.\n",
        "  - **Overcoming Local Minima**: By adding this fraction of previous updates, momentum effectively pushes the model out of small local minima, as it has a built-up force from earlier steps. This accumulated force can allow the model to escape minor dips in the loss function and continue searching for the global minimum.\n",
        "\n",
        "Thus, the role of momentum is crucial in preventing the optimization process from getting stuck in small, suboptimal solutions and accelerating convergence toward the global optimum."
      ],
      "metadata": {
        "id": "eIh-LMqjwt0c"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Avoiding Local Minima**\n"
      ],
      "metadata": {
        "id": "GCXh7iYMZ1IG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Gradual Reduction of Learning Rate**:\n",
        "  - Gradually reducing the learning rate over time can help avoid local minima and lead to a better global solution.\n",
        "  - **Learning Rate Scheduling**: Decrease the learning rate based on training progress. Common methods include exponential decay and step decay.\n",
        "  - **MATLAB Example for Learning Rate Reduction**:\n",
        "    ```matlab\n",
        "    % Define learning rate schedule options\n",
        "    options = trainingOptions('sgdm', ...\n",
        "        'InitialLearnRate', 0.1, ...\n",
        "        'LearnRateSchedule', 'piecewise', ...\n",
        "        'LearnRateDropFactor', 0.5, ...\n",
        "        'LearnRateDropPeriod', 20, ...\n",
        "        'MaxEpochs', 100);\n",
        "    \n",
        "    % Train the network\n",
        "    net = trainNetwork(inputData, targetData, layers, options);\n",
        "    ```\n",
        "    In this example, the learning rate is reduced by half every 20 epochs, which helps in escaping local minima.\n"
      ],
      "metadata": {
        "id": "zJ6QnpX7kxT7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Momentum Adjustment**:\n",
        "  - Dynamically adjusting momentum can also aid in avoiding local minima by allowing the network to adapt its step size based on the landscape of the loss function.\n",
        "  - High momentum during flat regions can speed up training, while reducing momentum near potential minima prevents overshooting.\n"
      ],
      "metadata": {
        "id": "OWlHmxdKkxLC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Batch Normalization**:\n",
        "  - Batch normalization can help smooth the optimization landscape, making it easier for the optimizer to avoid local minima.\n",
        "  - **MATLAB Example for Batch Normalization**:\n",
        "    ```matlab\n",
        "    % Define layers with batch normalization\n",
        "    layers = [\n",
        "        featureInputLayer(2)\n",
        "        fullyConnectedLayer(10)\n",
        "        batchNormalizationLayer\n",
        "        reluLayer\n",
        "        fullyConnectedLayer(1)\n",
        "        regressionLayer];\n",
        "    \n",
        "    % Train the network with batch normalization\n",
        "    options = trainingOptions('adam', 'MaxEpochs', 100);\n",
        "    net = trainNetwork(inputData, targetData, layers, options);\n",
        "    ```\n",
        "    Batch normalization normalizes the output of each layer, improving gradient flow and making it easier to converge without getting stuck in local minima.\n",
        "\n"
      ],
      "metadata": {
        "id": "x0iijlBlkw-q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Stopping Criteria**\n"
      ],
      "metadata": {
        "id": "zCbxy_0zZ1FT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Validation-Based Stopping**:\n",
        "  - The goal of stopping criteria is to prevent overfitting by stopping the training process when the model's performance on validation data stops improving.\n",
        "  - **Validation Patience**: Define a patience parameter that specifies the number of epochs to wait before stopping training if no improvement is observed.\n",
        "  - **MATLAB Example for Early Stopping**:\n",
        "    ```matlab\n",
        "    % Define training options with validation patience\n",
        "    options = trainingOptions('adam', ...\n",
        "        'InitialLearnRate', 0.01, ...\n",
        "        'MaxEpochs', 100, ...\n",
        "        'ValidationData', {validationInput, validationTarget}, ...\n",
        "        'ValidationFrequency', 5, ...\n",
        "        'ValidationPatience', 5);\n",
        "    \n",
        "    % Train the network\n",
        "    net = trainNetwork(inputData, targetData, layers, options);\n",
        "    ```\n",
        "    In this example, the training process stops if the validation error does not improve for 5 validation checks.\n"
      ],
      "metadata": {
        "id": "vPcCK7a4k7Ox"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Training Error Monitoring**:\n",
        "  - Monitor the training error and stop when a desired level of error is reached or when the rate of error reduction slows significantly.\n",
        "  - This ensures that the network is not overfitting and generalizes well to unseen data.\n"
      ],
      "metadata": {
        "id": "D5TP7nCMk7Ak"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Cross-Validation**:\n",
        "  - Cross-validation helps in determining the optimal stopping point by evaluating the model on different validation sets.\n",
        "  - Ensures that the model's performance is stable and not dependent on a specific subset of data.\n",
        "\n"
      ],
      "metadata": {
        "id": "JSHXZpQok6jC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Key Considerations for Training Neural Networks**\n",
        "  - **Hyperparameter Tuning**: Both learning rate and momentum are critical hyperparameters that need to be tuned to optimize training performance.\n",
        "  - **Avoiding Overfitting**: Use validation-based stopping and reduce the learning rate gradually to prevent the model from overfitting to the training data.\n",
        "  - **Adaptive Techniques**: Adaptive optimizers such as Adam and RMSprop dynamically adjust learning rates and momentum, making them popular choices for complex problems.\n",
        "\n"
      ],
      "metadata": {
        "id": "70ghbSU5fUtd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Validation and Testing in Neural Networks**\n",
        "\n"
      ],
      "metadata": {
        "id": "dfj-GMPoZ1Cv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Overview**\n",
        "  - Validation and testing are crucial stages in the development of neural networks to evaluate model performance and ensure generalization to unseen data.\n",
        "  - Validation helps in tuning hyperparameters and preventing overfitting, while testing provides a final evaluation of the model's performance.\n"
      ],
      "metadata": {
        "id": "7CBx0UQaZ0__"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## **Validation Process**\n"
      ],
      "metadata": {
        "id": "0TAsDCxqlH8d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Purpose of Validation**:\n",
        "  - Validation is used to evaluate the model's performance during training to tune hyperparameters and determine the point at which training should stop.\n",
        "  - It helps in ensuring that the model generalizes well to new, unseen data rather than simply memorizing the training set.\n"
      ],
      "metadata": {
        "id": "jQNDK6ZmlH6J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Validation Dataset**:\n",
        "  - A validation dataset is separate from both the training and test datasets.\n",
        "  - It is used during the training phase to monitor the model’s progress and make adjustments accordingly.\n"
      ],
      "metadata": {
        "id": "GmwyovDXlH3i"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Metrics for Validation**:\n",
        "  - **Accuracy**: The percentage of correctly classified samples.\n",
        "  - **Loss**: The value of the loss function calculated on the validation dataset to track overfitting or underfitting.\n",
        "  - **R² Score (Coefficient of Determination)**: Measures how well the predicted outcomes are correlated with actual outcomes.\n"
      ],
      "metadata": {
        "id": "lf003TmtlH09"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **MATLAB Example for Validation**:\n",
        "  ```matlab\n",
        "  % Define neural network layers\n",
        "  layers = [\n",
        "      featureInputLayer(2)\n",
        "      fullyConnectedLayer(10)\n",
        "      reluLayer\n",
        "      fullyConnectedLayer(1)\n",
        "      regressionLayer];\n",
        "  \n",
        "  % Split data into training, validation, and test sets\n",
        "  numData = length(inputData);\n",
        "  idx = randperm(numData);\n",
        "  trainIdx = idx(1:floor(0.7 * numData));\n",
        "  valIdx = idx(floor(0.7 * numData)+1:floor(0.85 * numData));\n",
        "  testIdx = idx(floor(0.85 * numData)+1:end);\n",
        "  \n",
        "  % Define training options with validation\n",
        "  options = trainingOptions('adam', ...\n",
        "      'InitialLearnRate', 0.01, ...\n",
        "      'MaxEpochs', 100, ...\n",
        "      'ValidationData', {inputData(valIdx, :), targetData(valIdx)}, ...\n",
        "      'ValidationFrequency', 10, ...\n",
        "      'Plots', 'training-progress');\n",
        "  \n",
        "  % Train the network\n",
        "  net = trainNetwork(inputData(trainIdx, :), targetData(trainIdx), layers, options);\n",
        "  ```\n",
        "  In this example, the training process includes a validation dataset that is monitored every 10 epochs to track the model's performance.\n",
        "\n"
      ],
      "metadata": {
        "id": "aPT1LUaAlHw8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Overfitting**\n"
      ],
      "metadata": {
        "id": "6VeceHahZ09k"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Definition of Overfitting**:\n",
        "  - Overfitting occurs when a model performs well on the training data but poorly on validation or test data due to excessive complexity or memorization.\n"
      ],
      "metadata": {
        "id": "htklWnTqlebb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **When to know the model is overfitting?**"
      ],
      "metadata": {
        "id": "nffjEmZOxliA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1. **Increasing Gap Between Training and Validation Accuracy:**\n",
        "   - If the training accuracy keeps improving but the validation accuracy plateaus or decreases, it’s a sign that the model is overfitting. The model is performing well on the training data but failing to generalize to new, unseen data (validation set).\n",
        "\n",
        "#### 2. **Validation Loss Starts Increasing:**\n",
        "   - Overfitting can be detected when the validation loss starts to increase while the training loss continues to decrease. This means the model is fitting too closely to the training data, including noise and outliers, and is not generalizing well to the validation set.\n",
        "\n",
        "#### 3. **High Complexity Model:**\n",
        "   - Overfitting is more likely to occur when the model is too complex (e.g., having too many parameters or layers relative to the size of the training data). A very complex model can capture noise in the training data, leading to overfitting.\n",
        "\n",
        "#### 4. **Low Bias, High Variance:**\n",
        "   - If your model shows low bias (i.e., small training error) but high variance (i.e., large difference between training and validation errors), it suggests overfitting. The model is too flexible, capturing specifics of the training data that don’t generalize well.\n",
        "\n",
        "#### 5. **Unstable Predictions:**\n",
        "   - A model that overfits may have unstable predictions when given small variations in the input data. For instance, if small changes in input features result in large swings in the output, the model is likely overfitting to specific patterns in the training data that don't generalize well.\n",
        "\n",
        "#### 6. **Excessive Training Time:**\n",
        "   - If you keep training for too many epochs without stopping early (early stopping), the model is more likely to overfit as it tries to memorize the training data rather than generalizing from it. Monitoring the performance on the validation set can help prevent this.\n",
        "\n",
        "#### 7. **Cross-Validation Performance:**\n",
        "   - Using cross-validation (e.g., k-fold cross-validation), if the model performs well on the training folds but significantly worse on the test folds, this indicates overfitting. The model is learning too much from the training set and fails to generalize to the test set.\n",
        "\n",
        "#### 8. **Overly Confident Predictions:**\n",
        "   - When a model is overfitting, it may output overly confident predictions (i.e., probabilities very close to 0 or 1) for the training data, but the confidence drops or is incorrect on the validation set. This is often an indicator that the model is memorizing the training data rather than learning generalizable patterns.\n",
        "\n",
        "#### 9. **Noise Sensitivity:**\n",
        "   - Overfitting models tend to be more sensitive to noise in the input data. Introducing small amounts of noise to the input should not drastically change the performance of a well-generalized model, but for an overfitted model, performance may drop significantly.\n"
      ],
      "metadata": {
        "id": "w9itmop4xq4Z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Technique to avoid overfitting?"
      ],
      "metadata": {
        "id": "CiNdbf-ex2Nn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Early Stopping**:\n",
        "  - Monitor the validation loss during training.\n",
        "  - Stop training when the validation loss stops decreasing, even if the training loss is still decreasing.\n",
        "  - Prevents the model from overfitting by stopping when it starts to learn noise instead of meaningful patterns.\n",
        "  - **MATLAB Example for Early Stopping**:\n",
        "    ```matlab\n",
        "    % Define training options with early stopping\n",
        "    options = trainingOptions('adam', ...\n",
        "        'InitialLearnRate', 0.01, ...\n",
        "        'MaxEpochs', 200, ...\n",
        "        'ValidationData', {inputData(valIdx, :), targetData(valIdx)}, ...\n",
        "        'ValidationFrequency', 10, ...\n",
        "        'ValidationPatience', 5, ... % Stops if no improvement for 5 checks\n",
        "        'Plots', 'training-progress');\n",
        "    \n",
        "    % Train the network\n",
        "    net = trainNetwork(inputData(trainIdx, :), targetData(trainIdx), layers, options);\n",
        "    ```\n",
        "    The example above uses validation patience, which stops training after 5 validation checks without improvement.\n"
      ],
      "metadata": {
        "id": "ICi6TTgLleY2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### **Regularization Techniques**:\n"
      ],
      "metadata": {
        "id": "GaD36fRcleWH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **L2 Regularization (Weight Decay)**: Adds a penalty to the loss function proportional to the sum of squared weights to reduce model complexity.\n"
      ],
      "metadata": {
        "id": "LLSzchvnleTl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **Dropout**: Randomly drops neurons during training to prevent the model from becoming too reliant on specific neurons, thereby enhancing generalization.\n"
      ],
      "metadata": {
        "id": "8-TSAJA2leP7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### **MATLAB Example for Dropout**:\n",
        "  ```matlab\n",
        "  % Define neural network layers with dropout\n",
        "  layers = [\n",
        "      featureInputLayer(2)\n",
        "      fullyConnectedLayer(10)\n",
        "      reluLayer\n",
        "      dropoutLayer(0.5) % Dropout layer with 50% rate\n",
        "      fullyConnectedLayer(1)\n",
        "      regressionLayer];\n",
        "  \n",
        "  % Train the network\n",
        "  options = trainingOptions('adam', 'MaxEpochs', 100, 'Plots', 'training-progress');\n",
        "  net = trainNetwork(inputData(trainIdx, :), targetData(trainIdx), layers, options);\n",
        "  ```\n",
        "  The example shows how to add a dropout layer to help reduce overfitting by dropping 50% of the neurons randomly during training.\n",
        "\n"
      ],
      "metadata": {
        "id": "acDC6gCsleLy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Testing Process**\n"
      ],
      "metadata": {
        "id": "YUAZO5DqZ06v"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Purpose of Testing**:\n",
        "  - Testing is conducted after the model has been trained and validated to evaluate its final performance on a completely unseen dataset.\n",
        "  - It provides an unbiased estimate of how well the model will generalize to real-world data.\n"
      ],
      "metadata": {
        "id": "5oO2BAggleJW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Test Dataset**:\n",
        "  - The test dataset must not have been used in training or validation to ensure accurate evaluation.\n"
      ],
      "metadata": {
        "id": "zc7clFCylr4m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Metrics for Testing**:\n",
        "  - **Accuracy**: The percentage of correct predictions out of total predictions.\n",
        "  - **Mean Squared Error (MSE)**: Used in regression tasks to measure the average squared difference between predictions and actual values.\n",
        "  - **Confusion Matrix**: For classification tasks, a confusion matrix provides insight into the types of errors made.\n",
        "  - **MATLAB Example for Testing**:\n",
        "    ```matlab\n",
        "    % Use the trained network to make predictions on the test set\n",
        "    predictions = predict(net, inputData(testIdx, :));\n",
        "    \n",
        "    % Calculate mean squared error (MSE) for regression\n",
        "    mseError = mean((predictions - targetData(testIdx)).^2);\n",
        "    fprintf('Mean Squared Error on Test Set: %f\\n', mseError);\n",
        "    ```\n",
        "    In this example, the trained network's performance is evaluated on the test set using mean squared error.\n",
        "\n"
      ],
      "metadata": {
        "id": "XCXR8A0slduK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## **Cross-Validation**\n"
      ],
      "metadata": {
        "id": "VJDKSegsZ04V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Purpose of Cross-Validation**:\n",
        "  - Cross-validation helps in evaluating the robustness of the model by splitting the data into multiple folds.\n",
        "  - Ensures that the model's performance is stable across different subsets of data.\n"
      ],
      "metadata": {
        "id": "ieLjW-8BlzD_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **K-Fold Cross-Validation**:\n",
        "  - The dataset is split into K equally sized subsets, and the model is trained K times, each time using a different subset as the validation set while the rest are used for training.\n",
        "  - The results are averaged to obtain a final performance measure.\n",
        "  - **MATLAB Example for Cross-Validation**:\n",
        "    ```matlab\n",
        "    % Define 5-fold cross-validation\n",
        "    k = 5;\n",
        "    cv = cvpartition(size(inputData, 1), 'KFold', k);\n",
        "    mseErrors = zeros(k, 1);\n",
        "    \n",
        "    % Perform cross-validation\n",
        "    for i = 1:k\n",
        "        trainIdx = training(cv, i);\n",
        "        valIdx = test(cv, i);\n",
        "        \n",
        "        % Train the network\n",
        "        net = trainNetwork(inputData(trainIdx, :), targetData(trainIdx), layers, options);\n",
        "        \n",
        "        % Validate the network\n",
        "        predictions = predict(net, inputData(valIdx, :));\n",
        "        mseErrors(i) = mean((predictions - targetData(valIdx)).^2);\n",
        "    end\n",
        "    \n",
        "    % Average MSE across all folds\n",
        "    avgMSE = mean(mseErrors);\n",
        "    fprintf('Average Mean Squared Error across folds: %f\\n', avgMSE);\n",
        "    ```\n",
        "    This example demonstrates how to perform K-fold cross-validation to evaluate model stability and generalization.\n",
        "\n"
      ],
      "metadata": {
        "id": "wekIGMLwlyjM"
      }
    }
  ]
}